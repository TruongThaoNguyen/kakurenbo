{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wCu1m4TTeCyb"
      },
      "source": [
        "# Subset selection for Semi-supervised learning (SSL)\n",
        "\n",
        "In this tutorial, we will look at an example showing how to integrate RETRIEVEDataloader with custom SSL training loop"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nRBrJb8I_vUv"
      },
      "source": [
        "### Cloning CORDS repository"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x35Mfc-RnKkX",
        "outputId": "ef215df4-af29-439d-9df0-0b4ad5f82dad"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cloning into 'cords'...\n",
            "remote: Enumerating objects: 5077, done.\u001b[K\n",
            "remote: Counting objects: 100% (392/392), done.\u001b[K\n",
            "remote: Compressing objects: 100% (147/147), done.\u001b[K\n",
            "remote: Total 5077 (delta 293), reused 294 (delta 238), pack-reused 4685\u001b[K\n",
            "Receiving objects: 100% (5077/5077), 58.45 MiB | 19.59 MiB/s, done.\n",
            "Resolving deltas: 100% (3144/3144), done.\n",
            "/content/cords\n",
            "\u001b[0m\u001b[01;34mbenchmarks\u001b[0m/   \u001b[01;34mdocs\u001b[0m/        \u001b[01;34mrequirements\u001b[0m/  train_sl.py\n",
            "CITATION.CFF  \u001b[01;34mexamples\u001b[0m/    setup.py       train_ssl.py\n",
            "\u001b[01;34mconfigs\u001b[0m/      LICENSE.txt  \u001b[01;34mtests\u001b[0m/         transformers_train_sl.py\n",
            "\u001b[01;34mcords\u001b[0m/        README.md    train_hpo.py   \u001b[01;34mtutorial\u001b[0m/\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/decile-team/cords.git\n",
        "%cd cords/\n",
        "%ls"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gAA3K0cVnyd9"
      },
      "source": [
        "### Install prerequisite libraries of CORDS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "041f23a8-2ae9-4ef7-9011-0b52d4ceb4fb",
        "id": "jLWChI_Ld9IW"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting dotmap\n",
            "  Downloading dotmap-1.3.30-py3-none-any.whl (11 kB)\n",
            "Installing collected packages: dotmap\n",
            "Successfully installed dotmap-1.3.30\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting apricot-select\n",
            "  Downloading apricot-select-0.6.1.tar.gz (28 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy>=1.14.2 in /usr/local/lib/python3.8/dist-packages (from apricot-select) (1.21.6)\n",
            "Requirement already satisfied: scipy>=1.0.0 in /usr/local/lib/python3.8/dist-packages (from apricot-select) (1.7.3)\n",
            "Requirement already satisfied: numba>=0.43.0 in /usr/local/lib/python3.8/dist-packages (from apricot-select) (0.56.4)\n",
            "Requirement already satisfied: tqdm>=4.24.0 in /usr/local/lib/python3.8/dist-packages (from apricot-select) (4.64.1)\n",
            "Collecting nose\n",
            "  Downloading nose-1.3.7-py3-none-any.whl (154 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m154.7/154.7 KB\u001b[0m \u001b[31m9.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: setuptools in /usr/local/lib/python3.8/dist-packages (from numba>=0.43.0->apricot-select) (57.4.0)\n",
            "Requirement already satisfied: llvmlite<0.40,>=0.39.0dev0 in /usr/local/lib/python3.8/dist-packages (from numba>=0.43.0->apricot-select) (0.39.1)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.8/dist-packages (from numba>=0.43.0->apricot-select) (6.0.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.8/dist-packages (from importlib-metadata->numba>=0.43.0->apricot-select) (3.12.0)\n",
            "Building wheels for collected packages: apricot-select\n",
            "  Building wheel for apricot-select (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for apricot-select: filename=apricot_select-0.6.1-py3-none-any.whl size=48786 sha256=a2c5763cead9d961c8a5b1b319f9fc3f815b310cea31c3646888056b81afda5b\n",
            "  Stored in directory: /root/.cache/pip/wheels/31/9d/60/56b3035d46924261240d200e1b3e99094ad23f2223a6d58b49\n",
            "Successfully built apricot-select\n",
            "Installing collected packages: nose, apricot-select\n",
            "Successfully installed apricot-select-0.6.1 nose-1.3.7\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting ray[default]\n",
            "  Downloading ray-2.2.0-cp38-cp38-manylinux2014_x86_64.whl (57.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.4/57.4 MB\u001b[0m \u001b[31m16.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: attrs in /usr/local/lib/python3.8/dist-packages (from ray[default]) (22.2.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.8/dist-packages (from ray[default]) (3.9.0)\n",
            "Requirement already satisfied: jsonschema in /usr/local/lib/python3.8/dist-packages (from ray[default]) (4.3.3)\n",
            "Requirement already satisfied: protobuf!=3.19.5,>=3.15.3 in /usr/local/lib/python3.8/dist-packages (from ray[default]) (3.19.6)\n",
            "Requirement already satisfied: numpy>=1.16 in /usr/local/lib/python3.8/dist-packages (from ray[default]) (1.21.6)\n",
            "Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.8/dist-packages (from ray[default]) (7.1.2)\n",
            "Requirement already satisfied: msgpack<2.0.0,>=1.0.0 in /usr/local/lib/python3.8/dist-packages (from ray[default]) (1.0.4)\n",
            "Requirement already satisfied: frozenlist in /usr/local/lib/python3.8/dist-packages (from ray[default]) (1.3.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from ray[default]) (2.25.1)\n",
            "Collecting virtualenv>=20.0.24\n",
            "  Downloading virtualenv-20.18.0-py3-none-any.whl (8.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.7/8.7 MB\u001b[0m \u001b[31m102.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pyyaml in /usr/local/lib/python3.8/dist-packages (from ray[default]) (6.0)\n",
            "Requirement already satisfied: grpcio>=1.32.0 in /usr/local/lib/python3.8/dist-packages (from ray[default]) (1.51.1)\n",
            "Requirement already satisfied: aiosignal in /usr/local/lib/python3.8/dist-packages (from ray[default]) (1.3.1)\n",
            "Requirement already satisfied: aiohttp>=3.7 in /usr/local/lib/python3.8/dist-packages (from ray[default]) (3.8.3)\n",
            "Collecting opencensus\n",
            "  Downloading opencensus-0.11.1-py2.py3-none-any.whl (128 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m128.2/128.2 KB\u001b[0m \u001b[31m17.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting prometheus-client<0.14.0,>=0.7.1\n",
            "  Downloading prometheus_client-0.13.1-py3-none-any.whl (57 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.1/57.1 KB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting aiohttp-cors\n",
            "  Downloading aiohttp_cors-0.7.0-py3-none-any.whl (27 kB)\n",
            "Requirement already satisfied: smart-open in /usr/local/lib/python3.8/dist-packages (from ray[default]) (6.3.0)\n",
            "Requirement already satisfied: pydantic in /usr/local/lib/python3.8/dist-packages (from ray[default]) (1.10.4)\n",
            "Collecting py-spy>=0.2.0\n",
            "  Downloading py_spy-0.3.14-py2.py3-none-manylinux_2_5_x86_64.manylinux1_x86_64.whl (3.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m89.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting colorful\n",
            "  Downloading colorful-0.5.5-py2.py3-none-any.whl (201 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m201.4/201.4 KB\u001b[0m \u001b[31m24.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting gpustat>=1.0.0\n",
            "  Downloading gpustat-1.0.0.tar.gz (90 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m90.5/90.5 KB\u001b[0m \u001b[31m13.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp>=3.7->ray[default]) (1.8.2)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.8/dist-packages (from aiohttp>=3.7->ray[default]) (6.0.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.8/dist-packages (from aiohttp>=3.7->ray[default]) (4.0.2)\n",
            "Requirement already satisfied: charset-normalizer<3.0,>=2.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp>=3.7->ray[default]) (2.1.1)\n",
            "Requirement already satisfied: six>=1.7 in /usr/local/lib/python3.8/dist-packages (from gpustat>=1.0.0->ray[default]) (1.15.0)\n",
            "Collecting nvidia-ml-py<=11.495.46,>=11.450.129\n",
            "  Downloading nvidia_ml_py-11.495.46-py3-none-any.whl (25 kB)\n",
            "Collecting psutil>=5.6.0\n",
            "  Downloading psutil-5.9.4-cp36-abi3-manylinux_2_12_x86_64.manylinux2010_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (280 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m280.2/280.2 KB\u001b[0m \u001b[31m31.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting blessed>=1.17.1\n",
            "  Downloading blessed-1.20.0-py2.py3-none-any.whl (58 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.4/58.4 KB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting distlib<1,>=0.3.6\n",
            "  Downloading distlib-0.3.6-py2.py3-none-any.whl (468 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m468.5/468.5 KB\u001b[0m \u001b[31m46.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: platformdirs<3,>=2.4 in /usr/local/lib/python3.8/dist-packages (from virtualenv>=20.0.24->ray[default]) (2.6.2)\n",
            "Requirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in /usr/local/lib/python3.8/dist-packages (from jsonschema->ray[default]) (0.19.3)\n",
            "Requirement already satisfied: importlib-resources>=1.4.0 in /usr/local/lib/python3.8/dist-packages (from jsonschema->ray[default]) (5.10.2)\n",
            "Requirement already satisfied: google-api-core<3.0.0,>=1.0.0 in /usr/local/lib/python3.8/dist-packages (from opencensus->ray[default]) (2.11.0)\n",
            "Collecting opencensus-context>=0.1.3\n",
            "  Downloading opencensus_context-0.1.3-py2.py3-none-any.whl (5.1 kB)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.8/dist-packages (from pydantic->ray[default]) (4.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->ray[default]) (2022.12.7)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests->ray[default]) (4.0.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->ray[default]) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->ray[default]) (2.10)\n",
            "Requirement already satisfied: wcwidth>=0.1.4 in /usr/local/lib/python3.8/dist-packages (from blessed>=1.17.1->gpustat>=1.0.0->ray[default]) (0.2.6)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0dev,>=1.56.2 in /usr/local/lib/python3.8/dist-packages (from google-api-core<3.0.0,>=1.0.0->opencensus->ray[default]) (1.58.0)\n",
            "Requirement already satisfied: google-auth<3.0dev,>=2.14.1 in /usr/local/lib/python3.8/dist-packages (from google-api-core<3.0.0,>=1.0.0->opencensus->ray[default]) (2.16.0)\n",
            "Requirement already satisfied: zipp>=3.1.0 in /usr/local/lib/python3.8/dist-packages (from importlib-resources>=1.4.0->jsonschema->ray[default]) (3.12.0)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from google-auth<3.0dev,>=2.14.1->google-api-core<3.0.0,>=1.0.0->opencensus->ray[default]) (5.3.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.8/dist-packages (from google-auth<3.0dev,>=2.14.1->google-api-core<3.0.0,>=1.0.0->opencensus->ray[default]) (0.2.8)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.8/dist-packages (from google-auth<3.0dev,>=2.14.1->google-api-core<3.0.0,>=1.0.0->opencensus->ray[default]) (4.9)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.8/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3.0dev,>=2.14.1->google-api-core<3.0.0,>=1.0.0->opencensus->ray[default]) (0.4.8)\n",
            "Building wheels for collected packages: gpustat\n",
            "  Building wheel for gpustat (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gpustat: filename=gpustat-1.0.0-py3-none-any.whl size=19887 sha256=4645cb1a7fd8634a321b355ca471dab7f922a370a91828b4bf0fbb7b304e6d6f\n",
            "  Stored in directory: /root/.cache/pip/wheels/1b/ed/14/0d513c962b25da841c42022cb5847c2ef835902c8563b8fb01\n",
            "Successfully built gpustat\n",
            "Installing collected packages: py-spy, opencensus-context, nvidia-ml-py, distlib, colorful, virtualenv, psutil, prometheus-client, blessed, gpustat, ray, aiohttp-cors, opencensus\n",
            "  Attempting uninstall: psutil\n",
            "    Found existing installation: psutil 5.4.8\n",
            "    Uninstalling psutil-5.4.8:\n",
            "      Successfully uninstalled psutil-5.4.8\n",
            "  Attempting uninstall: prometheus-client\n",
            "    Found existing installation: prometheus-client 0.16.0\n",
            "    Uninstalling prometheus-client-0.16.0:\n",
            "      Successfully uninstalled prometheus-client-0.16.0\n",
            "Successfully installed aiohttp-cors-0.7.0 blessed-1.20.0 colorful-0.5.5 distlib-0.3.6 gpustat-1.0.0 nvidia-ml-py-11.495.46 opencensus-0.11.1 opencensus-context-0.1.3 prometheus-client-0.13.1 psutil-5.9.4 py-spy-0.3.14 ray-2.2.0 virtualenv-20.18.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "psutil"
                ]
              }
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: ray[tune] in /usr/local/lib/python3.8/dist-packages (2.2.0)\n",
            "Requirement already satisfied: frozenlist in /usr/local/lib/python3.8/dist-packages (from ray[tune]) (1.3.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.8/dist-packages (from ray[tune]) (3.9.0)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.8/dist-packages (from ray[tune]) (6.0)\n",
            "Requirement already satisfied: protobuf!=3.19.5,>=3.15.3 in /usr/local/lib/python3.8/dist-packages (from ray[tune]) (3.19.6)\n",
            "Requirement already satisfied: attrs in /usr/local/lib/python3.8/dist-packages (from ray[tune]) (22.2.0)\n",
            "Requirement already satisfied: virtualenv>=20.0.24 in /usr/local/lib/python3.8/dist-packages (from ray[tune]) (20.18.0)\n",
            "Requirement already satisfied: grpcio>=1.32.0 in /usr/local/lib/python3.8/dist-packages (from ray[tune]) (1.51.1)\n",
            "Requirement already satisfied: msgpack<2.0.0,>=1.0.0 in /usr/local/lib/python3.8/dist-packages (from ray[tune]) (1.0.4)\n",
            "Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.8/dist-packages (from ray[tune]) (7.1.2)\n",
            "Requirement already satisfied: jsonschema in /usr/local/lib/python3.8/dist-packages (from ray[tune]) (4.3.3)\n",
            "Requirement already satisfied: aiosignal in /usr/local/lib/python3.8/dist-packages (from ray[tune]) (1.3.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from ray[tune]) (2.25.1)\n",
            "Requirement already satisfied: numpy>=1.16 in /usr/local/lib/python3.8/dist-packages (from ray[tune]) (1.21.6)\n",
            "Collecting tensorboardX>=1.9\n",
            "  Downloading tensorboardX-2.5.1-py2.py3-none-any.whl (125 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m125.4/125.4 KB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.8/dist-packages (from ray[tune]) (1.3.5)\n",
            "Requirement already satisfied: tabulate in /usr/local/lib/python3.8/dist-packages (from ray[tune]) (0.8.10)\n",
            "Requirement already satisfied: distlib<1,>=0.3.6 in /usr/local/lib/python3.8/dist-packages (from virtualenv>=20.0.24->ray[tune]) (0.3.6)\n",
            "Requirement already satisfied: platformdirs<3,>=2.4 in /usr/local/lib/python3.8/dist-packages (from virtualenv>=20.0.24->ray[tune]) (2.6.2)\n",
            "Requirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in /usr/local/lib/python3.8/dist-packages (from jsonschema->ray[tune]) (0.19.3)\n",
            "Requirement already satisfied: importlib-resources>=1.4.0 in /usr/local/lib/python3.8/dist-packages (from jsonschema->ray[tune]) (5.10.2)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.8/dist-packages (from pandas->ray[tune]) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.8/dist-packages (from pandas->ray[tune]) (2022.7.1)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->ray[tune]) (1.24.3)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests->ray[tune]) (4.0.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->ray[tune]) (2022.12.7)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->ray[tune]) (2.10)\n",
            "Requirement already satisfied: zipp>=3.1.0 in /usr/local/lib/python3.8/dist-packages (from importlib-resources>=1.4.0->jsonschema->ray[tune]) (3.12.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.8/dist-packages (from python-dateutil>=2.7.3->pandas->ray[tune]) (1.15.0)\n",
            "Installing collected packages: tensorboardX\n",
            "Successfully installed tensorboardX-2.5.1\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting datasets\n",
            "  Downloading datasets-2.9.0-py3-none-any.whl (462 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m462.8/462.8 KB\u001b[0m \u001b[31m12.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.8/dist-packages (from datasets) (2.25.1)\n",
            "Collecting responses<0.19\n",
            "  Downloading responses-0.18.0-py3-none-any.whl (38 kB)\n",
            "Collecting huggingface-hub<1.0.0,>=0.2.0\n",
            "  Downloading huggingface_hub-0.12.0-py3-none-any.whl (190 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m190.3/190.3 KB\u001b[0m \u001b[31m24.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.8/dist-packages (from datasets) (6.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.8/dist-packages (from datasets) (3.8.3)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.8/dist-packages (from datasets) (4.64.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.8/dist-packages (from datasets) (1.3.5)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.8/dist-packages (from datasets) (23.0)\n",
            "Collecting xxhash\n",
            "  Downloading xxhash-3.2.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (213 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m213.0/213.0 KB\u001b[0m \u001b[31m28.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting multiprocess\n",
            "  Downloading multiprocess-0.70.14-py38-none-any.whl (132 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m132.0/132.0 KB\u001b[0m \u001b[31m17.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: fsspec[http]>=2021.11.1 in /usr/local/lib/python3.8/dist-packages (from datasets) (2023.1.0)\n",
            "Requirement already satisfied: dill<0.3.7 in /usr/local/lib/python3.8/dist-packages (from datasets) (0.3.6)\n",
            "Requirement already satisfied: pyarrow>=6.0.0 in /usr/local/lib/python3.8/dist-packages (from datasets) (9.0.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.8/dist-packages (from datasets) (1.21.6)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets) (4.0.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets) (1.8.2)\n",
            "Requirement already satisfied: charset-normalizer<3.0,>=2.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets) (2.1.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets) (6.0.4)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets) (1.3.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets) (22.2.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.8/dist-packages (from huggingface-hub<1.0.0,>=0.2.0->datasets) (4.4.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.8/dist-packages (from huggingface-hub<1.0.0,>=0.2.0->datasets) (3.9.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests>=2.19.0->datasets) (2.10)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests>=2.19.0->datasets) (4.0.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests>=2.19.0->datasets) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests>=2.19.0->datasets) (2022.12.7)\n",
            "Collecting urllib3<1.27,>=1.21.1\n",
            "  Downloading urllib3-1.26.14-py2.py3-none-any.whl (140 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m140.6/140.6 KB\u001b[0m \u001b[31m18.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.8/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.8/dist-packages (from pandas->datasets) (2022.7.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.8/dist-packages (from python-dateutil>=2.7.3->pandas->datasets) (1.15.0)\n",
            "Installing collected packages: xxhash, urllib3, multiprocess, responses, huggingface-hub, datasets\n",
            "  Attempting uninstall: urllib3\n",
            "    Found existing installation: urllib3 1.24.3\n",
            "    Uninstalling urllib3-1.24.3:\n",
            "      Successfully uninstalled urllib3-1.24.3\n",
            "Successfully installed datasets-2.9.0 huggingface-hub-0.12.0 multiprocess-0.70.14 responses-0.18.0 urllib3-1.26.14 xxhash-3.2.0\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting transformers\n",
            "  Downloading transformers-4.26.0-py3-none-any.whl (6.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.3/6.3 MB\u001b[0m \u001b[31m57.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.8/dist-packages (from transformers) (3.9.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.8/dist-packages (from transformers) (2022.6.2)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.8/dist-packages (from transformers) (1.21.6)\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1\n",
            "  Downloading tokenizers-0.13.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.6/7.6 MB\u001b[0m \u001b[31m103.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.8/dist-packages (from transformers) (4.64.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.8/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from transformers) (2.25.1)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.8/dist-packages (from transformers) (23.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.11.0 in /usr/local/lib/python3.8/dist-packages (from transformers) (0.12.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.8/dist-packages (from huggingface-hub<1.0,>=0.11.0->transformers) (4.4.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (1.26.14)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (2022.12.7)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (4.0.0)\n",
            "Installing collected packages: tokenizers, transformers\n",
            "Successfully installed tokenizers-0.13.2 transformers-4.26.0\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting sentence-transformers\n",
            "  Downloading sentence-transformers-2.2.2.tar.gz (85 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.0/86.0 KB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.6.0 in /usr/local/lib/python3.8/dist-packages (from sentence-transformers) (4.26.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.8/dist-packages (from sentence-transformers) (4.64.1)\n",
            "Requirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.8/dist-packages (from sentence-transformers) (1.13.1+cu116)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.8/dist-packages (from sentence-transformers) (0.14.1+cu116)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.8/dist-packages (from sentence-transformers) (1.21.6)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.8/dist-packages (from sentence-transformers) (1.0.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.8/dist-packages (from sentence-transformers) (1.7.3)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.8/dist-packages (from sentence-transformers) (3.7)\n",
            "Collecting sentencepiece\n",
            "  Downloading sentencepiece-0.1.97-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m33.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: huggingface-hub>=0.4.0 in /usr/local/lib/python3.8/dist-packages (from sentence-transformers) (0.12.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.8/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers) (3.9.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers) (2.25.1)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.8/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers) (23.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.8/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers) (6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.8/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers) (4.4.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.8/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (2022.6.2)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.8/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (0.13.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.8/dist-packages (from nltk->sentence-transformers) (1.2.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.8/dist-packages (from nltk->sentence-transformers) (7.1.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from scikit-learn->sentence-transformers) (3.1.0)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.8/dist-packages (from torchvision->sentence-transformers) (7.1.2)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (1.26.14)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (2022.12.7)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (4.0.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (2.10)\n",
            "Building wheels for collected packages: sentence-transformers\n",
            "  Building wheel for sentence-transformers (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sentence-transformers: filename=sentence_transformers-2.2.2-py3-none-any.whl size=125938 sha256=50c2f7497891312617f169b6070c06f47d7576d171361ae9d0828270d7bdc93b\n",
            "  Stored in directory: /root/.cache/pip/wheels/5e/6f/8c/d88aec621f3f542d26fac0342bef5e693335d125f4e54aeffe\n",
            "Successfully built sentence-transformers\n",
            "Installing collected packages: sentencepiece, sentence-transformers\n",
            "Successfully installed sentence-transformers-2.2.2 sentencepiece-0.1.97\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.8/dist-packages (1.0.2)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.8/dist-packages (from scikit-learn) (1.2.0)\n",
            "Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.8/dist-packages (from scikit-learn) (1.7.3)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from scikit-learn) (3.1.0)\n",
            "Requirement already satisfied: numpy>=1.14.6 in /usr/local/lib/python3.8/dist-packages (from scikit-learn) (1.21.6)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting wandb\n",
            "  Downloading wandb-0.13.9-py2.py3-none-any.whl (2.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m33.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: protobuf!=4.21.0,<5,>=3.12.0 in /usr/local/lib/python3.8/dist-packages (from wandb) (3.19.6)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.8/dist-packages (from wandb) (5.9.4)\n",
            "Requirement already satisfied: Click!=8.0.0,>=7.0 in /usr/local/lib/python3.8/dist-packages (from wandb) (7.1.2)\n",
            "Collecting sentry-sdk>=1.0.0\n",
            "  Downloading sentry_sdk-1.14.0-py2.py3-none-any.whl (178 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m178.9/178.9 KB\u001b[0m \u001b[31m23.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting docker-pycreds>=0.4.0\n",
            "  Downloading docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.8/dist-packages (from wandb) (57.4.0)\n",
            "Requirement already satisfied: appdirs>=1.4.3 in /usr/local/lib/python3.8/dist-packages (from wandb) (1.4.4)\n",
            "Collecting GitPython>=1.0.0\n",
            "  Downloading GitPython-3.1.30-py3-none-any.whl (184 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m184.0/184.0 KB\u001b[0m \u001b[31m23.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pathtools\n",
            "  Downloading pathtools-0.1.2.tar.gz (11 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.8/dist-packages (from wandb) (4.4.0)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.8/dist-packages (from wandb) (6.0)\n",
            "Requirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from wandb) (2.25.1)\n",
            "Collecting setproctitle\n",
            "  Downloading setproctitle-1.3.2-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (31 kB)\n",
            "Requirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.8/dist-packages (from docker-pycreds>=0.4.0->wandb) (1.15.0)\n",
            "Collecting gitdb<5,>=4.0.1\n",
            "  Downloading gitdb-4.0.10-py3-none-any.whl (62 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 KB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests<3,>=2.0.0->wandb) (2022.12.7)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests<3,>=2.0.0->wandb) (4.0.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests<3,>=2.0.0->wandb) (2.10)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests<3,>=2.0.0->wandb) (1.26.14)\n",
            "Collecting smmap<6,>=3.0.1\n",
            "  Downloading smmap-5.0.0-py3-none-any.whl (24 kB)\n",
            "Building wheels for collected packages: pathtools\n",
            "  Building wheel for pathtools (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pathtools: filename=pathtools-0.1.2-py3-none-any.whl size=8806 sha256=494125e3fa0fb5068c8fdb4ce12ed7c858b906eb2fc670f22156e8e07ac4c444\n",
            "  Stored in directory: /root/.cache/pip/wheels/4c/8e/7e/72fbc243e1aeecae64a96875432e70d4e92f3d2d18123be004\n",
            "Successfully built pathtools\n",
            "Installing collected packages: pathtools, smmap, setproctitle, sentry-sdk, docker-pycreds, gitdb, GitPython, wandb\n",
            "Successfully installed GitPython-3.1.30 docker-pycreds-0.4.0 gitdb-4.0.10 pathtools-0.1.2 sentry-sdk-1.14.0 setproctitle-1.3.2 smmap-5.0.0 wandb-0.13.9\n"
          ]
        }
      ],
      "source": [
        "!pip install dotmap\n",
        "!pip install apricot-select\n",
        "!pip install ray[default]\n",
        "!pip install ray[tune]\n",
        "!pip install datasets\n",
        "!pip install transformers\n",
        "!pip install sentence-transformers\n",
        "!pip install scikit-learn\n",
        "!pip install wandb"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hGPvq3LZDoNl"
      },
      "source": [
        "# Install Submodlib"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ARGprtqmDnys",
        "outputId": "cf7d26ee-ecf6-4d3a-bf6f-132200b2ddeb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n",
            "Cloning into 'submodlib'...\n",
            "remote: Enumerating objects: 2563, done.\u001b[K\n",
            "remote: Counting objects: 100% (4/4), done.\u001b[K\n",
            "remote: Compressing objects: 100% (4/4), done.\u001b[K\n",
            "remote: Total 2563 (delta 0), reused 0 (delta 0), pack-reused 2559\u001b[K\n",
            "Receiving objects: 100% (2563/2563), 30.56 MiB | 22.10 MiB/s, done.\n",
            "Resolving deltas: 100% (1909/1909), done.\n",
            "/content/submodlib\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Processing /content/submodlib\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numba in /usr/local/lib/python3.8/dist-packages (from submodlib==1.1.5) (0.56.4)\n",
            "Collecting numpy==1.20.1\n",
            "  Downloading numpy-1.20.1-cp38-cp38-manylinux2010_x86_64.whl (15.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m15.4/15.4 MB\u001b[0m \u001b[31m52.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting sklearn\n",
            "  Downloading sklearn-0.0.post1.tar.gz (3.6 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.8/dist-packages (from submodlib==1.1.5) (1.7.3)\n",
            "Requirement already satisfied: llvmlite<0.40,>=0.39.0dev0 in /usr/local/lib/python3.8/dist-packages (from numba->submodlib==1.1.5) (0.39.1)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.8/dist-packages (from numba->submodlib==1.1.5) (6.0.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.8/dist-packages (from numba->submodlib==1.1.5) (57.4.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.8/dist-packages (from importlib-metadata->numba->submodlib==1.1.5) (3.12.0)\n",
            "Building wheels for collected packages: submodlib, sklearn\n",
            "  Building wheel for submodlib (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for submodlib: filename=submodlib-1.1.5-cp38-cp38-linux_x86_64.whl size=523302 sha256=dec8d3e548b895977a2e6c4d766d18ffcb399bd1f36241c2d19b5bf516635764\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-xe4yjxpz/wheels/fd/7d/9b/2f261fe9459159cc1d6a3d049b17f6871ef9f89e0d7da9966d\n",
            "  Building wheel for sklearn (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sklearn: filename=sklearn-0.0.post1-py3-none-any.whl size=2344 sha256=3a9bd813f36afb1aeeb5c34763149a4f34fdcc7316e98d8ecea19b75eef70394\n",
            "  Stored in directory: /root/.cache/pip/wheels/14/25/f7/1cc0956978ae479e75140219088deb7a36f60459df242b1a72\n",
            "Successfully built submodlib sklearn\n",
            "Installing collected packages: sklearn, numpy, submodlib\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 1.21.6\n",
            "    Uninstalling numpy-1.21.6:\n",
            "      Successfully uninstalled numpy-1.21.6\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "cmdstanpy 1.1.0 requires numpy>=1.21, but you have numpy 1.20.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed numpy-1.20.1 sklearn-0.0.post1 submodlib-1.1.5\n",
            "/content/cords\n"
          ]
        }
      ],
      "source": [
        "%cd ..\n",
        "!git clone https://github.com/decile-team/submodlib.git\n",
        "%cd submodlib\n",
        "!pip install .\n",
        "%cd ../cords"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gzyCsbnJn3_L"
      },
      "source": [
        "###Import necessary libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GazQv21pjIKd"
      },
      "outputs": [],
      "source": [
        "import logging\n",
        "import numpy, random, time, json, copy\n",
        "import numpy as np\n",
        "import os.path as osp\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, Subset\n",
        "from cords.utils.data.data_utils import WeightedSubset\n",
        "from cords.utils.models import WideResNet, ShakeNet, CNN13, CNN\n",
        "from cords.utils.data.datasets.SSL import utils as dataset_utils\n",
        "from cords.selectionstrategies.helpers.ssl_lib.algs import utils as alg_utils\n",
        "from cords.utils.models import utils as model_utils\n",
        "from cords.utils.data.datasets.SSL import gen_dataset\n",
        "from cords.selectionstrategies.helpers.ssl_lib.param_scheduler import scheduler\n",
        "from cords.selectionstrategies.helpers.ssl_lib.misc.meter import Meter\n",
        "from cords.utils.config_utils import load_config_data\n",
        "import time\n",
        "import os\n",
        "import sys"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uYOP5EWU_UD7"
      },
      "source": [
        "###Get logger object for logging"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LZ8H3_Hx_TtF"
      },
      "outputs": [],
      "source": [
        "def __get_logger(results_dir):\n",
        "  os.makedirs(results_dir, exist_ok=True)\n",
        "  # setup logger\n",
        "  plain_formatter = logging.Formatter(\"[%(asctime)s] %(name)s %(levelname)s: %(message)s\",\n",
        "                                      datefmt=\"%m/%d %H:%M:%S\")\n",
        "  logger = logging.getLogger(__name__)\n",
        "  logger.setLevel(logging.INFO)\n",
        "  s_handler = logging.StreamHandler(stream=sys.stdout)\n",
        "  s_handler.setFormatter(plain_formatter)\n",
        "  s_handler.setLevel(logging.INFO)\n",
        "  logger.addHandler(s_handler)\n",
        "  f_handler = logging.FileHandler(os.path.join(results_dir, \"results.log\"))\n",
        "  f_handler.setFormatter(plain_formatter)\n",
        "  f_handler.setLevel(logging.DEBUG)\n",
        "  logger.addHandler(f_handler)\n",
        "  logger.propagate = False\n",
        "  return logger\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bwUZJRQxBLyx"
      },
      "source": [
        "### Defining the results directory and getting the results logger object"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NUSLouwCBLRB"
      },
      "outputs": [],
      "source": [
        "results_dir = 'results/'\n",
        "logger = __get_logger(results_dir)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MyMJdoeqok49"
      },
      "source": [
        "### Loading configuration file with predefined arguments:\n",
        "\n",
        "We have a set of predefined configuration files added to CORDS for SSL under cords/configs/SSL/ which can be used directly by loading them as a dotmap object. \n",
        "\n",
        "An example of predefined configuration for CIFAR10 using VAT as SSL algorithm and RETRIEVE as subset selection strategy can be found below:\n",
        "\n",
        "```Python3\n",
        "config = dict(setting=\"SSL\",\n",
        "              dataset=dict(name=\"cifar10\",\n",
        "                           root=\"../data\",\n",
        "                           feature=\"dss\",\n",
        "                           type=\"pre-defined\",\n",
        "                           num_labels=4000,\n",
        "                           val_ratio=0.1,\n",
        "                           ood_ratio=0.5,\n",
        "                           random_split=False,\n",
        "                           whiten=False,\n",
        "                           zca=True,\n",
        "                           labeled_aug='WA',\n",
        "                           unlabeled_aug='WA',\n",
        "                           wa='t.t.f',\n",
        "                           strong_aug=False),\n",
        "\n",
        "              dataloader=dict(shuffle=True,\n",
        "                              pin_memory=True,\n",
        "                              num_workers=8,\n",
        "                              l_batch_size=50,\n",
        "                              ul_batch_size=50),\n",
        "\n",
        "              model=dict(architecture='wrn',\n",
        "                         type='pre-defined',\n",
        "                         numclasses=10),\n",
        "\n",
        "              ckpt=dict(is_load=False,\n",
        "                        is_save=True,\n",
        "                        checkpoint_model='model.ckpt',\n",
        "                        checkpoint_optimizer='optimizer.ckpt',\n",
        "                        start_iter=None,\n",
        "                        checkpoint=10000),\n",
        "\n",
        "              loss=dict(type='CrossEntropyLoss',\n",
        "                        use_sigmoid=False),\n",
        "\n",
        "              optimizer=dict(type=\"sgd\",\n",
        "                             momentum=0.9,\n",
        "                             lr=0.03,\n",
        "                             weight_decay=0,\n",
        "                             nesterov=True,\n",
        "                             tsa=False,\n",
        "                             tsa_schedule='linear'),\n",
        "\n",
        "              scheduler=dict(lr_decay=\"cos\",\n",
        "                             warmup_iter=0),\n",
        "\n",
        "              ssl_args=dict(alg='vat',\n",
        "                            coef=0.3,\n",
        "                            ema_teacher=False,\n",
        "                            ema_teacher_warmup=False,\n",
        "                            ema_teacher_factor=0.999,\n",
        "                            ema_apply_wd=False,\n",
        "                            em=0,\n",
        "                            threshold=None,\n",
        "                            sharpen=None,\n",
        "                            temp_softmax=None,\n",
        "                            consis='ce',\n",
        "                            eps=6,\n",
        "                            xi=1e-6,\n",
        "                            vat_iter=1\n",
        "                            ),\n",
        "\n",
        "              ssl_eval_args=dict(weight_average=False,\n",
        "                                 wa_ema_factor=0.999,\n",
        "                                 wa_apply_wd=False),\n",
        "\n",
        "              dss_args=dict(type=\"RETRIEVE-Warm\",\n",
        "                            fraction=0.1,\n",
        "                            select_every=20,\n",
        "                            kappa=0.5,\n",
        "                            linear_layer=False,\n",
        "                            selection_type='Supervised',\n",
        "                            greedy='Stochastic',\n",
        "                            valid=True),\n",
        "\n",
        "              train_args=dict(iteration=500000,\n",
        "                              max_iter=-1,\n",
        "                              device=\"cuda\",\n",
        "                              results_dir='results/',\n",
        "                              disp=256,\n",
        "                              seed=96)\n",
        "              )\n",
        "\n",
        "```\n",
        "\n",
        "Please find a detailed documentation explaining the available configuration parameters in the following readthedocs [page]()\n",
        "\n",
        "***Loading the predefined configuration file directly using the load_config_data function in CORDS***"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vozeGsg3CenF"
      },
      "outputs": [],
      "source": [
        "from cords.utils.config_utils import load_config_data\n",
        "cfg = load_config_data('/content/cords/configs/SSL/config_retrieve-warm_vat_cifar10.py')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b54OTNMpBqm7"
      },
      "source": [
        "### Loading the CIFAR10 dataset for SSL\n",
        "\n",
        "Since CIFAR10 dataset is a predefined dataset in CORDS repository for SSL. You can use the gen_dataset function in cords/utils/data/datasets/SSL/builder.py for loading the CIFAR10 dataset.\n",
        "\n",
        "**Input parameters of gen_dataset function:**\n",
        "\n",
        "Parameters\n",
        "-----------\n",
        "    root: str\n",
        "        root directory in which data is present or needs to be downloaded\n",
        "    dataset: str\n",
        "        dataset name,\n",
        "        Existing dataset choices: ['cifar10', 'cifar100', 'svhn', 'stl10', 'cifarOOD', 'mnistOOD', 'cifarImbalance']\n",
        "    validation_split: bool\n",
        "        if True, return validation loader.\n",
        "        We use 10% random split of training data as validation data\n",
        "    cfg: argparse.Namespace or dict\n",
        "        Dictionary containing necessary arguments for generating the dataset\n",
        "    logger: logging.Logger\n",
        "        Logger class for logging the information\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 500,
          "referenced_widgets": [
            "484fedf4fb1d4096b94b4a6c0dcb8404",
            "639eafff1225470d8e5deae79ff51688",
            "efc76f5d95b042109ef6d5b9e1d25d7d",
            "f973931cc1144eb682d19f7bf75b05e3",
            "d6349c6388814efa83ffb477a93ee986",
            "b383e1bb4724406b9a69cbaf29676c4d",
            "9879665b0e5d4bfbbcf2dd3ed36a810b",
            "75401a58ef2a4c2883e8505d74c2d0fe",
            "0ce29baa7efa4ec89f35443105e24a0f",
            "28aacca06cf746fc9f2ee35b78939190",
            "2a6c5eaed1454183bf5bfd0b7da0236d"
          ]
        },
        "id": "rkjRkzs2olSD",
        "outputId": "44fbaed0-dc2c-4a2d-f3bc-1b695390b8c5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to data/cifar-10-python.tar.gz\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "484fedf4fb1d4096b94b4a6c0dcb8404",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/170498071 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting data/cifar-10-python.tar.gz to data/\n",
            "[02/06 01:40:36] __main__ INFO: number of :\n",
            "             training data: 50000\n",
            "             labeled data: 4000\n",
            "             unlabeled data: 50000\n",
            "             validation data: 0\n",
            "             test data: 10000\n",
            "[02/06 01:40:58] __main__ INFO: labeled augmentation\n",
            "[02/06 01:40:58] __main__ INFO: Compose(\n",
            "    ToPILImage()\n",
            "    RandomHorizontalFlip(p=0.5)\n",
            "    RandomCrop(size=(32, 32), padding=4)\n",
            "    ToTensor()\n",
            "    GCN(multiplier=55, eps=1e-10)\n",
            "    ZCA()\n",
            ")\n",
            "[02/06 01:40:58] __main__ INFO: unlabeled augmentation\n",
            "[02/06 01:40:58] __main__ INFO: Compose(\n",
            "    ToPILImage()\n",
            "    RandomHorizontalFlip(p=0.5)\n",
            "    RandomCrop(size=(32, 32), padding=4)\n",
            "    ToTensor()\n",
            "    GCN(multiplier=55, eps=1e-10)\n",
            "    ZCA()\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "lt_data, ult_data, test_data, num_classes, img_size = gen_dataset('data/', 'cifar10',\n",
        "                                                                  False, cfg, logger)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WGI8vbIF1IOS"
      },
      "source": [
        "###Defining Model\n",
        "\n",
        "CORDS has a set of predefined models bulit in utils folder. You can import them directly by passing on the corresponding set of rquired arguments for the model.\n",
        "\n",
        "In this notebook, we are going to use a WideResNet model that takes in the following arguments:\n",
        "\n",
        "```\n",
        "WideResNet Parameters\n",
        "-----------\n",
        "  num_classes: int\n",
        "      number of classes\n",
        "  filters: int\n",
        "      number of filters\n",
        "  scales: int\n",
        "      number of scales\n",
        "  repeat: int\n",
        "      number of residual blocks per scale\n",
        "  dropout: float\n",
        "      dropout ratio (None indicates dropout is unused)\n",
        "\n",
        "```\n",
        "\n",
        "We have numclasses which is a part of model arguments in the config file and can be accessed by cfg.model.numclasses\n",
        "\n",
        "---\n",
        "***NOTE***\n",
        "\n",
        "### Instead of as dictionary objects, we load config files as dotmap objects. Hence, we can use dot notation (e.g., cfg.model) or original dictionary notation (e.g., cfg['model']) to access the elements. However, we suggest the usage of dot notation for consistency purposes.\n",
        "---\n",
        "\n",
        "\n",
        "      "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f97m03ZbqvNK"
      },
      "outputs": [],
      "source": [
        "from cords.utils.models import WideResNet\n",
        "\n",
        "scale = int(np.ceil(np.log2(img_size)))\n",
        "\n",
        "#Defining the model and copies the model to the device mentioned in train_args.device argument in config file\n",
        "model = WideResNet(cfg.model.numclasses, 32, scale, 4).to(cfg.train_args.device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kY3HImbxbRQ3"
      },
      "source": [
        "### Defining Teacher Model\n",
        "\n",
        "Some SSL algorithms use a teacher model to estimate the consistency loss. We will be using the argument cfg.ssl_args.ema_teacher in the config file to denote as a boolean indicator for the usage of the teacher model. In our example, where we use the VAT algorithm, which does not use a teacher model. So, we can set the cfg.ssl_args.ema_teacher argument to be False.\n",
        "\n",
        "In cases where we use teacher model, we may need to mention additional arguments like cfg.ssl_args.ema_teacher_warmup and cfg.ssl_args.ema_teacher_factor which are specifically required for calculating the teacher model properties using exponential moving average."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w6doDzn1bQ9S"
      },
      "outputs": [],
      "source": [
        "# build teacher model\n",
        "scale = int(np.ceil(np.log2(img_size)))\n",
        "if cfg.ssl_args.ema_teacher:\n",
        "    teacher_model = WideResNet(cfg.model.numclasses, 32, scale, 4).to(cfg.train_args.device)\n",
        "    teacher_model.load_state_dict(model.state_dict())\n",
        "else:\n",
        "    teacher_model = None"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qmcH9ePeBjHT"
      },
      "source": [
        "### Defining Evaluation Model\n",
        "\n",
        "We can evaluate SSL algorithms on exponential moving average model or just on the model itself. We will be using the argument cfg.ssl_eval_args.weight_average in the config file to denote as a boolean indicator for the usage of the exponential weight average model for evaluation. In our example,\n",
        "we will not be using weight avearge for evaluation. So, we can set the cfg.ssl_eval_args.weight_average argument to be False.\n",
        "\n",
        "In cases where we use teacher model, we may need to mention additional arguments like cfg.ssl_args.ema_teacher_warmup and cfg.ssl_args.ema_teacher_factor which are specifically required for calculating the teacher model properties using exponential moving average."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EmIUf1VFc2u6"
      },
      "outputs": [],
      "source": [
        "# for evaluation\n",
        "scale = int(np.ceil(np.log2(img_size)))\n",
        "if cfg.ssl_eval_args.weight_average:\n",
        "    average_model = WideResNet(cfg.model.numclasses, 32, scale, 4).to(cfg.train_args.device)\n",
        "    average_model.load_state_dict(model.state_dict())\n",
        "else:\n",
        "    average_model = None\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SiKPjvOADqMd"
      },
      "source": [
        "### Get SSL consistency loss functions \n",
        "\n",
        "gen_consistency function is implemented in the following file 'cords/selectionstrategies/helpers/ssl_lib/consistency/builder file' and it can be imported as follows:\n",
        "```\n",
        "from cords.selectionstrategies.helpers.ssl_lib.consistency.builder import gen_consistency\n",
        "```\n",
        "Existing Consistency loss functions are:\n",
        "1.   Cross-Entropy Loss\n",
        "2.   Squared Loss\n",
        "\n",
        "---\n",
        "***Note:*** \n",
        "\n",
        "### We generate two versions of loss functions with mean reduction and without mean reduction. Loss function without mean reduction is used for data subset selection as most of the subset selection strategies need individual loss gradients. Hence, using a loss function without reduction helps calculate these individual loss gradients.\n",
        "---\n",
        "\n",
        "We will be using ssl_args configuration arguments for generating the consistency function.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zScOt6dHDo07"
      },
      "outputs": [],
      "source": [
        "from cords.selectionstrategies.helpers.ssl_lib.consistency.builder import gen_consistency\n",
        "\n",
        "consistency = gen_consistency(cfg.ssl_args.consis, cfg)\n",
        "consistency_nored = gen_consistency(cfg.ssl_args.consis + '_red', cfg)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QHhKdWovCelB"
      },
      "source": [
        "### Defining SSL algorithm\n",
        "\n",
        "We integrated various consistency based SSL algorithms implemented in this awesome [repository](https://github.com/perrying/pytorch-consistency-regularization) with cords. These SSL algorithms can be imported by using gen_ssl_alg function implemented in cords.selectionstrategies.helpers.ssl_lib.algs.builder which can be imported as follows:\n",
        "\n",
        "```\n",
        "from cords.selectionstrategies.helpers.ssl_lib.algs.builder import gen_ssl_alg\n",
        "```\n",
        "\n",
        "In our example, we will be using VAT as SSL algorithm."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n-8Z9zKbCdub"
      },
      "outputs": [],
      "source": [
        "from cords.selectionstrategies.helpers.ssl_lib.algs.builder import gen_ssl_alg\n",
        "\n",
        "ssl_alg = gen_ssl_alg(cfg.ssl_args.alg, cfg)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fwEl97QQFs2i"
      },
      "outputs": [],
      "source": [
        "max_iteration = int(cfg.train_args.iteration * cfg.dss_args.fraction)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lnL-sve1qrnP"
      },
      "source": [
        "### Create unlabeled, labeled and test dataloaders"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QnvXqaGbqnhH",
        "outputId": "f04cc8e6-b562-41dc-d292-a1935eb7a1e0"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py:554: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n"
          ]
        }
      ],
      "source": [
        "#Creating full unlabeled data loader with shuffle set to be False\n",
        "ult_seq_loader = DataLoader(ult_data, batch_size=cfg.dataloader.ul_batch_size,\n",
        "                                    shuffle=False, pin_memory=True)\n",
        "\n",
        "#Creating labeled data loader with shuffle set to be False\n",
        "lt_seq_loader = DataLoader(lt_data, batch_size=cfg.dataloader.l_batch_size,\n",
        "                            shuffle=False, pin_memory=True)\n",
        "\n",
        "#Creating test data loader with shuffle set to be False\n",
        "test_loader = DataLoader(\n",
        "    test_data,\n",
        "    1,\n",
        "    shuffle=False,\n",
        "    drop_last=False,\n",
        "    num_workers=cfg.dataloader.num_workers\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vq_ehn_0vPjZ"
      },
      "source": [
        "### Instantiating RETRIEVE subset selection dataloader for unlabeled data\n",
        "\n",
        "We instantiate subset dataloaders that can be used for training the models with adaptive subsets.\n",
        "\n",
        "Each subset dataloader needs data selection strategy arguments in the form of a dotmap dictionary, logger and dataloader specific arguments like batch size, shuffle etc. We will be using dss_args in config file along with some additional arguments required for RETRIEVE.\n",
        "\n",
        "Additional arguments required for RETRIEVEDataLoader on top of dss_args in the config file are:\n",
        "\n",
        "* model\n",
        "* teacher_model\n",
        "* ssl_alg\n",
        "* consistency_nored\n",
        "* num_classes\n",
        "* max_iteration\n",
        "* learning rate\n",
        "* device\n",
        "\n",
        "We are instantiating RETRIEVE dataloader here with warm start. But any dataloader can be instantiated in the same way by passing the required arguments\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8TNMpF36xykF",
        "outputId": "d038afda-2f4d-44d9-cd90-a4de3e8828a9"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py:554: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n"
          ]
        }
      ],
      "source": [
        "from cords.utils.data.dataloader.SSL.adaptive import RETRIEVEDataLoader\n",
        "from dotmap import DotMap\n",
        "\n",
        "cfg.dss_args.model = model\n",
        "cfg.dss_args.tea_model = teacher_model\n",
        "cfg.dss_args.ssl_alg = ssl_alg\n",
        "cfg.dss_args.loss = consistency_nored\n",
        "cfg.dss_args.num_classes = num_classes\n",
        "cfg.dss_args.num_iters = max_iteration\n",
        "cfg.dss_args.eta = cfg.optimizer.lr\n",
        "cfg.dss_args.device = cfg.train_args.device\n",
        "\n",
        "ult_loader = RETRIEVEDataLoader(ult_seq_loader, lt_seq_loader, cfg.dss_args, logger=logger,\n",
        "                                batch_size=cfg.dataloader.ul_batch_size,\n",
        "                                pin_memory=cfg.dataloader.pin_memory,\n",
        "                                num_workers=cfg.dataloader.num_workers)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f9TqCLfbS0Oz"
      },
      "source": [
        "### Get Optimizer\n",
        "\n",
        "We store optimizer related arguments in the optimizer option of the configuration file. In our example, we will be using \"sgd\" optimizer with Nesterov momentum without any weight decay. The config.optimizer arguments in our example are as follows:\n",
        "\n",
        "```\n",
        "optimizer=dict(type=\"sgd\",\n",
        "                momentum=0.9,\n",
        "                lr=0.03,\n",
        "                weight_decay=0,\n",
        "                nesterov=True,\n",
        "                tsa=False,\n",
        "                tsa_schedule='linear')\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MGq41g3aSzqn"
      },
      "outputs": [],
      "source": [
        "if cfg.optimizer.type == \"sgd\":\n",
        "    optimizer = optim.SGD(\n",
        "                model.parameters(), cfg.optimizer.lr, cfg.optimizer.momentum, \n",
        "                weight_decay=cfg.optimizer.weight_decay, nesterov=cfg.optimizer.nesterov)\n",
        "elif cfg.optimizer.type == \"adam\":\n",
        "    optimizer = optim.Adam(\n",
        "        model.parameters(), cfg.optimizer.lr, (cfg.optimizer.momentum, 0.999), \n",
        "        weight_decay=cfg.optimizer.weight_decay)\n",
        "else:\n",
        "    raise NotImplementedError\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QX-iP0FbXnVv"
      },
      "source": [
        "### Get Scheduler\n",
        "\n",
        "We store scheduler related arguments in the scheduler option of the configuration file. In our example, we will be using cosine-annealing scheduler. The config.scheduler arguments in our example are as follows:\n",
        "\n",
        "```\n",
        "scheduler=dict(lr_decay=\"cos\",\n",
        "              warmup_iter=0),\n",
        "\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-TzvQ2H-XinL"
      },
      "outputs": [],
      "source": [
        "# set lr scheduler\n",
        "if cfg.scheduler.lr_decay == \"cos\":\n",
        "    if cfg.dss_args.type == 'Full':\n",
        "        lr_scheduler = scheduler.CosineAnnealingLR(optimizer, max_iteration)\n",
        "    else:\n",
        "        lr_scheduler = scheduler.CosineAnnealingLR(optimizer,\n",
        "                                                    cfg.train_args.iteration * cfg.dss_args.fraction)\n",
        "elif cfg.scheduler.lr_decay == \"step\":\n",
        "    # TODO: fixed milestones\n",
        "    lr_scheduler = optim.lr_scheduler.MultiStepLR(optimizer, [400000, ], cfg.scheduler.lr_decay_rate)\n",
        "else:\n",
        "    raise NotImplementedError\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Ytyl3sgZzoU"
      },
      "source": [
        "### SSL Model Parameters Update function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JmrRh3S1ZuUt"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "############################## Model Parameters Update ##############################\n",
        "\"\"\"\n",
        "\n",
        "def param_update(cfg,\n",
        "                cur_iteration,\n",
        "                model,\n",
        "                teacher_model,\n",
        "                optimizer,\n",
        "                ssl_alg,\n",
        "                consistency,\n",
        "                labeled_data,\n",
        "                ul_weak_data,\n",
        "                ul_strong_data,\n",
        "                labels,\n",
        "                average_model,\n",
        "                weights=None,\n",
        "                ood=False\n",
        "                ):\n",
        "    start_time = time.time()\n",
        "    # Concantenate labeled data, weakly augmented, and strongly augmented unlabeled data\n",
        "    all_data = torch.cat([labeled_data, ul_weak_data, ul_strong_data], 0)\n",
        "    forward_func = model.forward\n",
        "    stu_logits = forward_func(all_data)\n",
        "    labeled_preds = stu_logits[:labeled_data.shape[0]]\n",
        "\n",
        "    # Separate weak unlabeled logits, and strong unlabeled logits\n",
        "    stu_unlabeled_weak_logits, stu_unlabeled_strong_logits = torch.chunk(stu_logits[labels.shape[0]:], 2, dim=0)\n",
        "    \n",
        "    # Use training signal annealing (TSA)\n",
        "    if cfg.optimizer.tsa:\n",
        "        none_reduced_loss = F.cross_entropy(labeled_preds, labels, reduction=\"none\")\n",
        "        L_supervised = alg_utils.anneal_loss(\n",
        "            labeled_preds, labels, none_reduced_loss, cur_iteration + 1,\n",
        "            cfg.train_args.iteration, labeled_preds.shape[1], cfg.optimizer.tsa_schedule)\n",
        "    else:\n",
        "        L_supervised = F.cross_entropy(labeled_preds, labels)\n",
        "\n",
        "    # IF SSL coefficient is greater than zero, calculate the consistency loss\n",
        "    if cfg.ssl_args.coef > 0:\n",
        "        # get target values\n",
        "        if teacher_model is not None:  # get target values from teacher model\n",
        "            t_forward_func = teacher_model.forward\n",
        "            tea_logits = t_forward_func(all_data)\n",
        "            tea_unlabeled_weak_logits, _ = torch.chunk(tea_logits[labels.shape[0]:], 2, dim=0)\n",
        "        else:\n",
        "            t_forward_func = forward_func\n",
        "            tea_unlabeled_weak_logits = stu_unlabeled_weak_logits\n",
        "\n",
        "        # calculate consistency loss\n",
        "        model.update_batch_stats(False)\n",
        "        y, targets, mask = ssl_alg(\n",
        "            stu_preds=stu_unlabeled_strong_logits,\n",
        "            tea_logits=tea_unlabeled_weak_logits.detach(),\n",
        "            w_data=ul_strong_data,\n",
        "            subset=False,\n",
        "            stu_forward=forward_func,\n",
        "            tea_forward=t_forward_func\n",
        "        )\n",
        "        model.update_batch_stats(True)\n",
        "\n",
        "        # calculate weighted consistency loss\n",
        "        if weights is None:\n",
        "            L_consistency = consistency(y, targets, mask, weak_prediction=tea_unlabeled_weak_logits.softmax(1))\n",
        "        else:\n",
        "            L_consistency = consistency(y, targets, mask * weights,\n",
        "                                        weak_prediction=tea_unlabeled_weak_logits.softmax(1))\n",
        "    else:\n",
        "        L_consistency = torch.zeros_like(L_supervised)\n",
        "        mask = None\n",
        "\n",
        "    # calculate total loss\n",
        "    coef = scheduler.exp_warmup(cfg.ssl_args.coef, int(cfg.scheduler.warmup_iter), cur_iteration + 1)\n",
        "    loss = L_supervised + coef * L_consistency\n",
        "    if cfg.ssl_args.em > 0:\n",
        "        loss -= cfg.ssl_args.em * \\\n",
        "                (stu_unlabeled_weak_logits.softmax(1) * F.log_softmax(stu_unlabeled_weak_logits, 1)).sum(1).mean()\n",
        "\n",
        "    # update parameters\n",
        "    cur_lr = optimizer.param_groups[0][\"lr\"]\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    if cfg.optimizer.weight_decay > 0:\n",
        "        decay_coeff = cfg.optimizer.weight_decay * cur_lr\n",
        "        model_utils.apply_weight_decay(model.modules(), decay_coeff)\n",
        "    optimizer.step()\n",
        "\n",
        "    # update teacher parameters by exponential moving average\n",
        "    if cfg.ssl_args.ema_teacher:\n",
        "        model_utils.ema_update(\n",
        "            teacher_model, model, cfg.ssl_args.ema_teacher_factor,\n",
        "            cfg.optimizer.weight_decay * cur_lr if cfg.ssl_args.ema_apply_wd else None,\n",
        "            cur_iteration if cfg.ssl_args.ema_teacher_warmup else None)\n",
        "    \n",
        "    # update evaluation model's parameters by exponential moving average\n",
        "    if cfg.ssl_eval_args.weight_average:\n",
        "        model_utils.ema_update(\n",
        "            average_model, model, cfg.ssl_eval_args.wa_ema_factor,\n",
        "            cfg.optimizer.weight_decay * cur_lr if cfg.ssl_eval_args.wa_apply_wd else None)\n",
        "\n",
        "    # calculate accuracy for labeled data\n",
        "    acc = (labeled_preds.max(1)[1] == labels).float().mean()\n",
        "\n",
        "    return {\n",
        "        \"acc\": acc,\n",
        "        \"loss\": loss.item(),\n",
        "        \"sup loss\": L_supervised.item(),\n",
        "        \"ssl loss\": L_consistency.item(),\n",
        "        \"mask\": mask.float().mean().item() if mask is not None else 1,\n",
        "        \"coef\": coef,\n",
        "        \"sec/iter\": (time.time() - start_time)\n",
        "    }\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y71bNVrDbbqj"
      },
      "source": [
        "### SSL model evaluation function\n",
        "\n",
        "Function that evaluates the raw SSL model and EMA evaluation model if any on test dataloader to calculate accuracy and loss metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FIiITLWKbbeX"
      },
      "outputs": [],
      "source": [
        "def evaluation(raw_model, eval_model, loader, device):\n",
        "    raw_model.eval()\n",
        "    eval_model.eval()\n",
        "    sum_raw_acc = sum_acc = sum_loss = 0\n",
        "    with torch.no_grad():\n",
        "        for (data, labels) in loader:\n",
        "            data, labels = data.to(device), labels.to(device)\n",
        "            preds = eval_model(data)\n",
        "            raw_preds = raw_model(data)\n",
        "            loss = F.cross_entropy(preds, labels)\n",
        "            sum_loss += loss.item()\n",
        "            acc = (preds.max(1)[1] == labels).float().mean()\n",
        "            raw_acc = (raw_preds.max(1)[1] == labels).float().mean()\n",
        "            sum_acc += acc.item()\n",
        "            sum_raw_acc += raw_acc.item()\n",
        "    mean_raw_acc = sum_raw_acc / len(loader)\n",
        "    mean_acc = sum_acc / len(loader)\n",
        "    mean_loss = sum_loss / len(loader)\n",
        "    raw_model.train()\n",
        "    eval_model.train()\n",
        "    return mean_raw_acc, mean_acc, mean_loss\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-oAMCMlISbd1"
      },
      "source": [
        "### SSL Training loop\n",
        "\n",
        "In SSL training loop, we iterate over batches of labeled and unlabeled data subset selected. We can do this by iterating over labeled and RETRIEVEDataloader as follows:\n",
        "\n",
        "```\n",
        "for batch_idx, (l_data, ul_data) in enumerate(zip(lt_loader, ult_loader)):\n",
        "  # ult_loader is an object of RETRIEVEDataloader class\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "j3SyrvRKSbE3",
        "outputId": "4d4a72f3-2cc3-40f8-dee1-dbd8c54b3c04"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[02/06 01:41:08] __main__ INFO: WideResNet(\n",
            "  (feature_extractor): Sequential(\n",
            "    (0): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "    (1): _Residual(\n",
            "      (pre_act): Sequential(\n",
            "        (0): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (1): LeakyReLU(negative_slope=0.1)\n",
            "      )\n",
            "      (identity): Conv2d(16, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (layer): Sequential(\n",
            "        (0): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (2): LeakyReLU(negative_slope=0.1)\n",
            "        (3): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      )\n",
            "    )\n",
            "    (2): _Residual(\n",
            "      (pre_act): Identity()\n",
            "      (identity): Identity()\n",
            "      (layer): Sequential(\n",
            "        (0): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (1): LeakyReLU(negative_slope=0.1)\n",
            "        (2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (4): LeakyReLU(negative_slope=0.1)\n",
            "        (5): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      )\n",
            "    )\n",
            "    (3): _Residual(\n",
            "      (pre_act): Identity()\n",
            "      (identity): Identity()\n",
            "      (layer): Sequential(\n",
            "        (0): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (1): LeakyReLU(negative_slope=0.1)\n",
            "        (2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (4): LeakyReLU(negative_slope=0.1)\n",
            "        (5): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      )\n",
            "    )\n",
            "    (4): _Residual(\n",
            "      (pre_act): Identity()\n",
            "      (identity): Identity()\n",
            "      (layer): Sequential(\n",
            "        (0): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (1): LeakyReLU(negative_slope=0.1)\n",
            "        (2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (4): LeakyReLU(negative_slope=0.1)\n",
            "        (5): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      )\n",
            "    )\n",
            "    (5): _Residual(\n",
            "      (pre_act): Identity()\n",
            "      (identity): Conv2d(32, 64, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "      (layer): Sequential(\n",
            "        (0): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (1): LeakyReLU(negative_slope=0.1)\n",
            "        (2): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "        (3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (4): LeakyReLU(negative_slope=0.1)\n",
            "        (5): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      )\n",
            "    )\n",
            "    (6): _Residual(\n",
            "      (pre_act): Identity()\n",
            "      (identity): Identity()\n",
            "      (layer): Sequential(\n",
            "        (0): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (1): LeakyReLU(negative_slope=0.1)\n",
            "        (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (4): LeakyReLU(negative_slope=0.1)\n",
            "        (5): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      )\n",
            "    )\n",
            "    (7): _Residual(\n",
            "      (pre_act): Identity()\n",
            "      (identity): Identity()\n",
            "      (layer): Sequential(\n",
            "        (0): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (1): LeakyReLU(negative_slope=0.1)\n",
            "        (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (4): LeakyReLU(negative_slope=0.1)\n",
            "        (5): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      )\n",
            "    )\n",
            "    (8): _Residual(\n",
            "      (pre_act): Identity()\n",
            "      (identity): Identity()\n",
            "      (layer): Sequential(\n",
            "        (0): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (1): LeakyReLU(negative_slope=0.1)\n",
            "        (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (4): LeakyReLU(negative_slope=0.1)\n",
            "        (5): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      )\n",
            "    )\n",
            "    (9): _Residual(\n",
            "      (pre_act): Identity()\n",
            "      (identity): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "      (layer): Sequential(\n",
            "        (0): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (1): LeakyReLU(negative_slope=0.1)\n",
            "        (2): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "        (3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (4): LeakyReLU(negative_slope=0.1)\n",
            "        (5): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      )\n",
            "    )\n",
            "    (10): _Residual(\n",
            "      (pre_act): Identity()\n",
            "      (identity): Identity()\n",
            "      (layer): Sequential(\n",
            "        (0): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (1): LeakyReLU(negative_slope=0.1)\n",
            "        (2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (4): LeakyReLU(negative_slope=0.1)\n",
            "        (5): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      )\n",
            "    )\n",
            "    (11): _Residual(\n",
            "      (pre_act): Identity()\n",
            "      (identity): Identity()\n",
            "      (layer): Sequential(\n",
            "        (0): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (1): LeakyReLU(negative_slope=0.1)\n",
            "        (2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (4): LeakyReLU(negative_slope=0.1)\n",
            "        (5): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      )\n",
            "    )\n",
            "    (12): _Residual(\n",
            "      (pre_act): Identity()\n",
            "      (identity): Identity()\n",
            "      (layer): Sequential(\n",
            "        (0): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (1): LeakyReLU(negative_slope=0.1)\n",
            "        (2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (4): LeakyReLU(negative_slope=0.1)\n",
            "        (5): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      )\n",
            "    )\n",
            "    (13): _Residual(\n",
            "      (pre_act): Identity()\n",
            "      (identity): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "      (layer): Sequential(\n",
            "        (0): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (1): LeakyReLU(negative_slope=0.1)\n",
            "        (2): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "        (3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (4): LeakyReLU(negative_slope=0.1)\n",
            "        (5): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      )\n",
            "    )\n",
            "    (14): _Residual(\n",
            "      (pre_act): Identity()\n",
            "      (identity): Identity()\n",
            "      (layer): Sequential(\n",
            "        (0): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (1): LeakyReLU(negative_slope=0.1)\n",
            "        (2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (4): LeakyReLU(negative_slope=0.1)\n",
            "        (5): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      )\n",
            "    )\n",
            "    (15): _Residual(\n",
            "      (pre_act): Identity()\n",
            "      (identity): Identity()\n",
            "      (layer): Sequential(\n",
            "        (0): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (1): LeakyReLU(negative_slope=0.1)\n",
            "        (2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (4): LeakyReLU(negative_slope=0.1)\n",
            "        (5): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      )\n",
            "    )\n",
            "    (16): _Residual(\n",
            "      (pre_act): Identity()\n",
            "      (identity): Identity()\n",
            "      (layer): Sequential(\n",
            "        (0): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (1): LeakyReLU(negative_slope=0.1)\n",
            "        (2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (4): LeakyReLU(negative_slope=0.1)\n",
            "        (5): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      )\n",
            "    )\n",
            "    (17): _Residual(\n",
            "      (pre_act): Identity()\n",
            "      (identity): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "      (layer): Sequential(\n",
            "        (0): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (1): LeakyReLU(negative_slope=0.1)\n",
            "        (2): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "        (3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (4): LeakyReLU(negative_slope=0.1)\n",
            "        (5): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      )\n",
            "    )\n",
            "    (18): _Residual(\n",
            "      (pre_act): Identity()\n",
            "      (identity): Identity()\n",
            "      (layer): Sequential(\n",
            "        (0): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (1): LeakyReLU(negative_slope=0.1)\n",
            "        (2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (4): LeakyReLU(negative_slope=0.1)\n",
            "        (5): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      )\n",
            "    )\n",
            "    (19): _Residual(\n",
            "      (pre_act): Identity()\n",
            "      (identity): Identity()\n",
            "      (layer): Sequential(\n",
            "        (0): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (1): LeakyReLU(negative_slope=0.1)\n",
            "        (2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (4): LeakyReLU(negative_slope=0.1)\n",
            "        (5): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      )\n",
            "    )\n",
            "    (20): _Residual(\n",
            "      (pre_act): Identity()\n",
            "      (identity): Identity()\n",
            "      (layer): Sequential(\n",
            "        (0): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (1): LeakyReLU(negative_slope=0.1)\n",
            "        (2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (4): LeakyReLU(negative_slope=0.1)\n",
            "        (5): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      )\n",
            "    )\n",
            "    (21): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (22): LeakyReLU(negative_slope=0.1)\n",
            "  )\n",
            "  (classifier): Sequential(\n",
            "    (0): Linear(in_features=512, out_features=10, bias=True)\n",
            "  )\n",
            ")\n",
            "[02/06 01:41:08] __main__ INFO: training\n",
            "[02/06 01:42:17] __main__ INFO: [256/50000] acc 0.480134 | loss 2.17131 | sup loss 1.48722 | ssl loss 2.2803 | mask 1 | sec/iter 0.222022 | ssl coef 0.3 | lr 0.03\n",
            "[02/06 01:43:15] __main__ INFO: [512/50000] acc 0.587809 | loss 1.77392 | sup loss 1.16462 | ssl loss 2.031 | mask 1 | sec/iter 0.217629 | ssl coef 0.3 | lr 0.03\n",
            "[02/06 01:44:12] __main__ INFO: [768/50000] acc 0.626508 | loss 1.64582 | sup loss 1.06485 | ssl loss 1.93657 | mask 1 | sec/iter 0.218378 | ssl coef 0.3 | lr 0.02999\n",
            "[02/06 01:45:12] __main__ INFO: [1024/50000] acc 0.667733 | loss 1.49544 | sup loss 0.935504 | ssl loss 1.86645 | mask 1 | sec/iter 0.226084 | ssl coef 0.3 | lr 0.02999\n",
            "[02/06 01:46:09] __main__ INFO: [1280/50000] acc 0.70322 | loss 1.37176 | sup loss 0.835688 | ssl loss 1.78691 | mask 1 | sec/iter 0.219055 | ssl coef 0.3 | lr 0.02998\n",
            "[02/06 01:47:07] __main__ INFO: [1536/50000] acc 0.740663 | loss 1.2426 | sup loss 0.72314 | ssl loss 1.73154 | mask 1 | sec/iter 0.220332 | ssl coef 0.3 | lr 0.02997\n",
            "[02/06 01:48:05] __main__ INFO: [1792/50000] acc 0.757701 | loss 1.18088 | sup loss 0.66388 | ssl loss 1.72334 | mask 1 | sec/iter 0.21712 | ssl coef 0.3 | lr 0.02996\n",
            "[02/06 01:49:04] __main__ INFO: [2048/50000] acc 0.771799 | loss 1.10091 | sup loss 0.595281 | ssl loss 1.68544 | mask 1 | sec/iter 0.21747 | ssl coef 0.3 | lr 0.02995\n",
            "[02/06 01:50:01] __main__ INFO: [2304/50000] acc 0.782079 | loss 1.0701 | sup loss 0.577372 | ssl loss 1.64244 | mask 1 | sec/iter 0.215748 | ssl coef 0.3 | lr 0.02994\n",
            "[02/06 01:50:59] __main__ INFO: [2560/50000] acc 0.842769 | loss 0.95803 | sup loss 0.465634 | ssl loss 1.64132 | mask 1 | sec/iter 0.215852 | ssl coef 0.3 | lr 0.02993\n",
            "[02/06 01:51:58] __main__ INFO: [2816/50000] acc 0.835477 | loss 0.924161 | sup loss 0.441715 | ssl loss 1.60815 | mask 1 | sec/iter 0.217534 | ssl coef 0.3 | lr 0.02991\n",
            "[02/06 01:52:55] __main__ INFO: [3072/50000] acc 0.870537 | loss 0.851131 | sup loss 0.383347 | ssl loss 1.55928 | mask 1 | sec/iter 0.214543 | ssl coef 0.3 | lr 0.02989\n",
            "[02/06 01:53:53] __main__ INFO: [3328/50000] acc 0.881557 | loss 0.791836 | sup loss 0.324093 | ssl loss 1.55914 | mask 1 | sec/iter 0.216597 | ssl coef 0.3 | lr 0.02987\n",
            "[02/06 01:54:52] __main__ INFO: [3584/50000] acc 0.888735 | loss 0.763785 | sup loss 0.309488 | ssl loss 1.51433 | mask 1 | sec/iter 0.219001 | ssl coef 0.3 | lr 0.02985\n",
            "[02/06 01:55:50] __main__ INFO: [3840/50000] acc 0.903025 | loss 0.725742 | sup loss 0.279903 | ssl loss 1.48613 | mask 1 | sec/iter 0.218448 | ssl coef 0.3 | lr 0.02983\n",
            "[02/06 01:56:47] __main__ INFO: [4096/50000] acc 0.89564 | loss 0.728388 | sup loss 0.271345 | ssl loss 1.52348 | mask 1 | sec/iter 0.217196 | ssl coef 0.3 | lr 0.02981\n",
            "[02/06 01:57:46] __main__ INFO: [4352/50000] acc 0.922578 | loss 0.657639 | sup loss 0.219904 | ssl loss 1.45912 | mask 1 | sec/iter 0.226385 | ssl coef 0.3 | lr 0.02979\n",
            "[02/06 01:58:44] __main__ INFO: [4608/50000] acc 0.948886 | loss 0.607119 | sup loss 0.165412 | ssl loss 1.47236 | mask 1 | sec/iter 0.216172 | ssl coef 0.3 | lr 0.02976\n",
            "[02/06 01:59:42] __main__ INFO: [4864/50000] acc 0.928388 | loss 0.639334 | sup loss 0.208676 | ssl loss 1.43553 | mask 1 | sec/iter 0.21465 | ssl coef 0.3 | lr 0.02973\n",
            "[02/06 02:00:39] __main__ INFO: [5120/50000] acc 0.948864 | loss 0.5872 | sup loss 0.143916 | ssl loss 1.47761 | mask 1 | sec/iter 0.215082 | ssl coef 0.3 | lr 0.0297\n",
            "[02/06 02:01:38] __main__ INFO: [5376/50000] acc 0.951627 | loss 0.568949 | sup loss 0.141263 | ssl loss 1.42562 | mask 1 | sec/iter 0.219799 | ssl coef 0.3 | lr 0.02967\n",
            "[02/06 02:02:35] __main__ INFO: [5632/50000] acc 0.955718 | loss 0.535242 | sup loss 0.120544 | ssl loss 1.38233 | mask 1 | sec/iter 0.220561 | ssl coef 0.3 | lr 0.02964\n",
            "[02/06 02:03:33] __main__ INFO: [5888/50000] acc 0.950753 | loss 0.560446 | sup loss 0.14036 | ssl loss 1.40028 | mask 1 | sec/iter 0.215179 | ssl coef 0.3 | lr 0.02961\n",
            "[02/06 02:04:32] __main__ INFO: [6144/50000] acc 0.965613 | loss 0.494742 | sup loss 0.0894536 | ssl loss 1.35096 | mask 1 | sec/iter 0.218181 | ssl coef 0.3 | lr 0.02957\n",
            "[02/06 02:05:30] __main__ INFO: [6400/50000] acc 0.96319 | loss 0.513704 | sup loss 0.108612 | ssl loss 1.35031 | mask 1 | sec/iter 0.218861 | ssl coef 0.3 | lr 0.02954\n",
            "[02/06 02:06:28] __main__ INFO: [6656/50000] acc 0.977804 | loss 0.474679 | sup loss 0.0789445 | ssl loss 1.31911 | mask 1 | sec/iter 0.219325 | ssl coef 0.3 | lr 0.0295\n",
            "[02/06 02:07:27] __main__ INFO: [6912/50000] acc 0.97254 | loss 0.461409 | sup loss 0.08334 | ssl loss 1.26023 | mask 1 | sec/iter 0.220755 | ssl coef 0.3 | lr 0.02946\n",
            "[02/06 02:08:25] __main__ INFO: [7168/50000] acc 0.973838 | loss 0.448869 | sup loss 0.0729441 | ssl loss 1.25308 | mask 1 | sec/iter 0.221837 | ssl coef 0.3 | lr 0.02942\n",
            "[02/06 02:09:22] __main__ INFO: [7424/50000] acc 0.972426 | loss 0.491922 | sup loss 0.0986693 | ssl loss 1.31084 | mask 1 | sec/iter 0.219237 | ssl coef 0.3 | lr 0.02938\n",
            "[02/06 02:10:21] __main__ INFO: [7680/50000] acc 0.974911 | loss 0.453488 | sup loss 0.0724567 | ssl loss 1.27011 | mask 1 | sec/iter 0.216449 | ssl coef 0.3 | lr 0.02933\n",
            "[02/06 02:11:20] __main__ INFO: [7936/50000] acc 0.983663 | loss 0.393867 | sup loss 0.0510069 | ssl loss 1.14287 | mask 1 | sec/iter 0.218884 | ssl coef 0.3 | lr 0.02929\n",
            "[02/06 02:12:17] __main__ INFO: [8192/50000] acc 0.980135 | loss 0.458579 | sup loss 0.0712798 | ssl loss 1.291 | mask 1 | sec/iter 0.21692 | ssl coef 0.3 | lr 0.02924\n",
            "[02/06 02:13:15] __main__ INFO: [8448/50000] acc 0.987846 | loss 0.397662 | sup loss 0.0475281 | ssl loss 1.16711 | mask 1 | sec/iter 0.219407 | ssl coef 0.3 | lr 0.0292\n",
            "[02/06 02:14:15] __main__ INFO: [8704/50000] acc 0.983724 | loss 0.42122 | sup loss 0.0458941 | ssl loss 1.25109 | mask 1 | sec/iter 0.217818 | ssl coef 0.3 | lr 0.02915\n",
            "[02/06 02:15:12] __main__ INFO: [8960/50000] acc 0.972412 | loss 0.45827 | sup loss 0.0922158 | ssl loss 1.22018 | mask 1 | sec/iter 0.216387 | ssl coef 0.3 | lr 0.0291\n",
            "[02/06 02:16:10] __main__ INFO: [9216/50000] acc 0.984731 | loss 0.380677 | sup loss 0.0368644 | ssl loss 1.14604 | mask 1 | sec/iter 0.217412 | ssl coef 0.3 | lr 0.02904\n",
            "[02/06 02:17:08] __main__ INFO: [9472/50000] acc 0.976871 | loss 0.417219 | sup loss 0.0599819 | ssl loss 1.19079 | mask 1 | sec/iter 0.219704 | ssl coef 0.3 | lr 0.02899\n",
            "[02/06 02:18:08] __main__ INFO: [9728/50000] acc 0.988005 | loss 0.427554 | sup loss 0.0441966 | ssl loss 1.27786 | mask 1 | sec/iter 0.225327 | ssl coef 0.3 | lr 0.02893\n",
            "[02/06 02:19:05] __main__ INFO: [9984/50000] acc 0.98678 | loss 0.398332 | sup loss 0.0460581 | ssl loss 1.17425 | mask 1 | sec/iter 0.21781 | ssl coef 0.3 | lr 0.02888\n",
            "[02/06 02:19:09] __main__ INFO: test\n",
            "[02/06 02:21:56] __main__ INFO: test loss 1.214532 | test acc. 0.761000 | raw acc. 0.761000\n",
            "[02/06 02:22:51] __main__ INFO: [10240/50000] acc 0.990492 | loss 0.388715 | sup loss 0.0337247 | ssl loss 1.1833 | mask 1 | sec/iter 0.21633 | ssl coef 0.3 | lr 0.02882\n",
            "[02/06 02:23:49] __main__ INFO: [10496/50000] acc 0.981574 | loss 0.395021 | sup loss 0.0441346 | ssl loss 1.16962 | mask 1 | sec/iter 0.217831 | ssl coef 0.3 | lr 0.02876\n",
            "[02/06 02:24:48] __main__ INFO: [10752/50000] acc 0.988482 | loss 0.373654 | sup loss 0.0338962 | ssl loss 1.13253 | mask 1 | sec/iter 0.216685 | ssl coef 0.3 | lr 0.0287\n",
            "[02/06 02:25:46] __main__ INFO: [11008/50000] acc 0.994914 | loss 0.363596 | sup loss 0.0216416 | ssl loss 1.13985 | mask 1 | sec/iter 0.218984 | ssl coef 0.3 | lr 0.02864\n",
            "[02/06 02:26:43] __main__ INFO: [11264/50000] acc 0.984698 | loss 0.376836 | sup loss 0.041145 | ssl loss 1.11897 | mask 1 | sec/iter 0.214885 | ssl coef 0.3 | lr 0.02857\n",
            "[02/06 02:27:42] __main__ INFO: [11520/50000] acc 0.991845 | loss 0.377063 | sup loss 0.0300843 | ssl loss 1.1566 | mask 1 | sec/iter 0.24257 | ssl coef 0.3 | lr 0.02851\n",
            "[02/06 02:28:41] __main__ INFO: [11776/50000] acc 0.994196 | loss 0.358421 | sup loss 0.0213994 | ssl loss 1.12341 | mask 1 | sec/iter 0.219536 | ssl coef 0.3 | lr 0.02844\n",
            "[02/06 02:29:38] __main__ INFO: [12032/50000] acc 0.984274 | loss 0.394395 | sup loss 0.0463003 | ssl loss 1.16031 | mask 1 | sec/iter 0.217843 | ssl coef 0.3 | lr 0.02837\n",
            "[02/06 02:30:36] __main__ INFO: [12288/50000] acc 0.994588 | loss 0.37017 | sup loss 0.0213046 | ssl loss 1.16289 | mask 1 | sec/iter 0.220073 | ssl coef 0.3 | lr 0.02831\n",
            "[02/06 02:31:36] __main__ INFO: [12544/50000] acc 0.987014 | loss 0.383163 | sup loss 0.0478629 | ssl loss 1.11767 | mask 1 | sec/iter 0.220662 | ssl coef 0.3 | lr 0.02823\n",
            "[02/06 02:32:33] __main__ INFO: [12800/50000] acc 0.996301 | loss 0.359046 | sup loss 0.0159117 | ssl loss 1.14378 | mask 1 | sec/iter 0.216936 | ssl coef 0.3 | lr 0.02816\n",
            "[02/06 02:33:31] __main__ INFO: [13056/50000] acc 0.994704 | loss 0.332938 | sup loss 0.0164841 | ssl loss 1.05485 | mask 1 | sec/iter 0.222443 | ssl coef 0.3 | lr 0.02809\n",
            "[02/06 02:34:30] __main__ INFO: [13312/50000] acc 0.990308 | loss 0.353592 | sup loss 0.0259482 | ssl loss 1.09215 | mask 1 | sec/iter 0.248739 | ssl coef 0.3 | lr 0.02801\n",
            "[02/06 02:35:28] __main__ INFO: [13568/50000] acc 0.99129 | loss 0.350766 | sup loss 0.0233887 | ssl loss 1.09126 | mask 1 | sec/iter 0.217895 | ssl coef 0.3 | lr 0.02794\n",
            "[02/06 02:36:26] __main__ INFO: [13824/50000] acc 0.993416 | loss 0.315343 | sup loss 0.0183574 | ssl loss 0.989951 | mask 1 | sec/iter 0.218265 | ssl coef 0.3 | lr 0.02786\n",
            "[02/06 02:37:24] __main__ INFO: [14080/50000] acc 0.987762 | loss 0.357207 | sup loss 0.0392206 | ssl loss 1.05995 | mask 1 | sec/iter 0.219115 | ssl coef 0.3 | lr 0.02778\n",
            "[02/06 02:38:23] __main__ INFO: [14336/50000] acc 0.995971 | loss 0.322285 | sup loss 0.0186055 | ssl loss 1.01227 | mask 1 | sec/iter 0.219038 | ssl coef 0.3 | lr 0.0277\n",
            "[02/06 02:39:20] __main__ INFO: [14592/50000] acc 0.999157 | loss 0.30655 | sup loss 0.00934344 | ssl loss 0.99069 | mask 1 | sec/iter 0.216647 | ssl coef 0.3 | lr 0.02762\n",
            "[02/06 02:40:18] __main__ INFO: [14848/50000] acc 0.989539 | loss 0.340307 | sup loss 0.0262561 | ssl loss 1.04684 | mask 1 | sec/iter 0.216761 | ssl coef 0.3 | lr 0.02754\n",
            "[02/06 02:41:16] __main__ INFO: [15104/50000] acc 0.998031 | loss 0.312817 | sup loss 0.0128135 | ssl loss 1.00001 | mask 1 | sec/iter 0.226225 | ssl coef 0.3 | lr 0.02745\n",
            "[02/06 02:42:15] __main__ INFO: [15360/50000] acc 0.995786 | loss 0.301358 | sup loss 0.0126861 | ssl loss 0.96224 | mask 1 | sec/iter 0.219694 | ssl coef 0.3 | lr 0.02737\n",
            "[02/06 02:43:12] __main__ INFO: [15616/50000] acc 0.99514 | loss 0.301435 | sup loss 0.0145572 | ssl loss 0.956258 | mask 1 | sec/iter 0.216207 | ssl coef 0.3 | lr 0.02728\n",
            "[02/06 02:44:10] __main__ INFO: [15872/50000] acc 0.989843 | loss 0.309363 | sup loss 0.0217933 | ssl loss 0.958565 | mask 1 | sec/iter 0.216911 | ssl coef 0.3 | lr 0.02719\n",
            "[02/06 02:45:09] __main__ INFO: [16128/50000] acc 0.997947 | loss 0.317038 | sup loss 0.00940161 | ssl loss 1.02545 | mask 1 | sec/iter 0.22091 | ssl coef 0.3 | lr 0.0271\n",
            "[02/06 02:46:07] __main__ INFO: [16384/50000] acc 0.992835 | loss 0.295461 | sup loss 0.0208729 | ssl loss 0.915292 | mask 1 | sec/iter 0.222465 | ssl coef 0.3 | lr 0.02701\n",
            "[02/06 02:47:05] __main__ INFO: [16640/50000] acc 0.996844 | loss 0.295143 | sup loss 0.0100105 | ssl loss 0.950441 | mask 1 | sec/iter 0.217978 | ssl coef 0.3 | lr 0.02692\n",
            "[02/06 02:48:02] __main__ INFO: [16896/50000] acc 0.999095 | loss 0.289037 | sup loss 0.00544844 | ssl loss 0.945295 | mask 1 | sec/iter 0.21552 | ssl coef 0.3 | lr 0.02682\n",
            "[02/06 02:49:02] __main__ INFO: [17152/50000] acc 0.996534 | loss 0.282507 | sup loss 0.0117524 | ssl loss 0.902516 | mask 1 | sec/iter 0.220875 | ssl coef 0.3 | lr 0.02673\n",
            "[02/06 02:49:59] __main__ INFO: [17408/50000] acc 0.990183 | loss 0.315094 | sup loss 0.0284296 | ssl loss 0.955547 | mask 1 | sec/iter 0.220374 | ssl coef 0.3 | lr 0.02663\n",
            "[02/06 02:50:57] __main__ INFO: [17664/50000] acc 0.995391 | loss 0.295612 | sup loss 0.0142889 | ssl loss 0.937743 | mask 1 | sec/iter 0.215908 | ssl coef 0.3 | lr 0.02653\n",
            "[02/06 02:51:56] __main__ INFO: [17920/50000] acc 0.995249 | loss 0.27632 | sup loss 0.0135077 | ssl loss 0.87604 | mask 1 | sec/iter 0.230024 | ssl coef 0.3 | lr 0.02643\n",
            "[02/06 02:52:54] __main__ INFO: [18176/50000] acc 0.996805 | loss 0.272388 | sup loss 0.011076 | ssl loss 0.871041 | mask 1 | sec/iter 0.217678 | ssl coef 0.3 | lr 0.02633\n",
            "[02/06 02:53:52] __main__ INFO: [18432/50000] acc 0.994432 | loss 0.296307 | sup loss 0.0162453 | ssl loss 0.933538 | mask 1 | sec/iter 0.21634 | ssl coef 0.3 | lr 0.02623\n",
            "[02/06 02:54:49] __main__ INFO: [18688/50000] acc 0.9991 | loss 0.290586 | sup loss 0.00476349 | ssl loss 0.95274 | mask 1 | sec/iter 0.217203 | ssl coef 0.3 | lr 0.02613\n",
            "[02/06 02:55:48] __main__ INFO: [18944/50000] acc 0.99866 | loss 0.266306 | sup loss 0.00562289 | ssl loss 0.868943 | mask 1 | sec/iter 0.220654 | ssl coef 0.3 | lr 0.02602\n",
            "[02/06 02:56:46] __main__ INFO: [19200/50000] acc 0.999542 | loss 0.262664 | sup loss 0.00490467 | ssl loss 0.859199 | mask 1 | sec/iter 0.219404 | ssl coef 0.3 | lr 0.02592\n",
            "[02/06 02:57:44] __main__ INFO: [19456/50000] acc 0.998655 | loss 0.269691 | sup loss 0.00867959 | ssl loss 0.870037 | mask 1 | sec/iter 0.218155 | ssl coef 0.3 | lr 0.02581\n",
            "[02/06 02:58:42] __main__ INFO: [19712/50000] acc 0.998821 | loss 0.245843 | sup loss 0.00358421 | ssl loss 0.807529 | mask 1 | sec/iter 0.235133 | ssl coef 0.3 | lr 0.0257\n",
            "[02/06 02:59:42] __main__ INFO: [19968/50000] acc 0.995148 | loss 0.279099 | sup loss 0.0112179 | ssl loss 0.892935 | mask 1 | sec/iter 0.22347 | ssl coef 0.3 | lr 0.02559\n",
            "[02/06 02:59:49] __main__ INFO: test\n",
            "[02/06 03:02:38] __main__ INFO: test loss 1.313580 | test acc. 0.771700 | raw acc. 0.771700\n",
            "[02/06 03:03:29] __main__ INFO: [20224/50000] acc 0.997428 | loss 0.283527 | sup loss 0.00627327 | ssl loss 0.924179 | mask 1 | sec/iter 0.218097 | ssl coef 0.3 | lr 0.02548\n",
            "[02/06 03:04:26] __main__ INFO: [20480/50000] acc 0.988807 | loss 0.291401 | sup loss 0.0394561 | ssl loss 0.839817 | mask 1 | sec/iter 0.220332 | ssl coef 0.3 | lr 0.02537\n",
            "[02/06 03:05:24] __main__ INFO: [20736/50000] acc 0.998361 | loss 0.264965 | sup loss 0.00505822 | ssl loss 0.866356 | mask 1 | sec/iter 0.217402 | ssl coef 0.3 | lr 0.02526\n",
            "[02/06 03:06:23] __main__ INFO: [20992/50000] acc 0.999086 | loss 0.237798 | sup loss 0.00315414 | ssl loss 0.782145 | mask 1 | sec/iter 0.218159 | ssl coef 0.3 | lr 0.02514\n",
            "[02/06 03:07:21] __main__ INFO: [21248/50000] acc 0.996745 | loss 0.252403 | sup loss 0.0105784 | ssl loss 0.806083 | mask 1 | sec/iter 0.217602 | ssl coef 0.3 | lr 0.02503\n",
            "[02/06 03:08:18] __main__ INFO: [21504/50000] acc 0.993095 | loss 0.251521 | sup loss 0.0128951 | ssl loss 0.795421 | mask 1 | sec/iter 0.219724 | ssl coef 0.3 | lr 0.02491\n",
            "[02/06 03:09:17] __main__ INFO: [21760/50000] acc 0.997926 | loss 0.247197 | sup loss 0.00476255 | ssl loss 0.808113 | mask 1 | sec/iter 0.21972 | ssl coef 0.3 | lr 0.02479\n",
            "[02/06 03:10:15] __main__ INFO: [22016/50000] acc 0.995523 | loss 0.268347 | sup loss 0.0141212 | ssl loss 0.847419 | mask 1 | sec/iter 0.218953 | ssl coef 0.3 | lr 0.02467\n",
            "[02/06 03:11:13] __main__ INFO: [22272/50000] acc 0.99776 | loss 0.268204 | sup loss 0.00989869 | ssl loss 0.861019 | mask 1 | sec/iter 0.21986 | ssl coef 0.3 | lr 0.02455\n",
            "[02/06 03:12:11] __main__ INFO: [22528/50000] acc 0.999359 | loss 0.23859 | sup loss 0.0046891 | ssl loss 0.779669 | mask 1 | sec/iter 0.223046 | ssl coef 0.3 | lr 0.02443\n",
            "[02/06 03:13:10] __main__ INFO: [22784/50000] acc 0.994007 | loss 0.259854 | sup loss 0.0202012 | ssl loss 0.798844 | mask 1 | sec/iter 0.22223 | ssl coef 0.3 | lr 0.02431\n",
            "[02/06 03:14:08] __main__ INFO: [23040/50000] acc 0.998325 | loss 0.24309 | sup loss 0.00435921 | ssl loss 0.795771 | mask 1 | sec/iter 0.219259 | ssl coef 0.3 | lr 0.02418\n",
            "[02/06 03:15:06] __main__ INFO: [23296/50000] acc 0.999959 | loss 0.240786 | sup loss 0.0025102 | ssl loss 0.794252 | mask 1 | sec/iter 0.222086 | ssl coef 0.3 | lr 0.02406\n",
            "[02/06 03:16:06] __main__ INFO: [23552/50000] acc 0.996565 | loss 0.251441 | sup loss 0.00959556 | ssl loss 0.80615 | mask 1 | sec/iter 0.221396 | ssl coef 0.3 | lr 0.02393\n",
            "[02/06 03:17:04] __main__ INFO: [23808/50000] acc 0.997074 | loss 0.230242 | sup loss 0.0034192 | ssl loss 0.756077 | mask 1 | sec/iter 0.219057 | ssl coef 0.3 | lr 0.0238\n",
            "[02/06 03:18:04] __main__ INFO: [24064/50000] acc 0.994778 | loss 0.22532 | sup loss 0.00757283 | ssl loss 0.725825 | mask 1 | sec/iter 0.221431 | ssl coef 0.3 | lr 0.02367\n",
            "[02/06 03:19:02] __main__ INFO: [24320/50000] acc 0.999036 | loss 0.248283 | sup loss 0.00261901 | ssl loss 0.81888 | mask 1 | sec/iter 0.220077 | ssl coef 0.3 | lr 0.02354\n",
            "[02/06 03:20:00] __main__ INFO: [24576/50000] acc 0.999473 | loss 0.237258 | sup loss 0.00206186 | ssl loss 0.783987 | mask 1 | sec/iter 0.218439 | ssl coef 0.3 | lr 0.02341\n",
            "[02/06 03:21:00] __main__ INFO: [24832/50000] acc 0.998726 | loss 0.224718 | sup loss 0.00577192 | ssl loss 0.72982 | mask 1 | sec/iter 0.218214 | ssl coef 0.3 | lr 0.02328\n",
            "[02/06 03:25:34] __main__ INFO: Iteration: 25001, subset selection finished, takes 235.43. \n",
            "[02/06 03:25:56] __main__ INFO: [25088/50000] acc 0.999946 | loss 0.212703 | sup loss 0.00274925 | ssl loss 0.699845 | mask 1 | sec/iter 0.226465 | ssl coef 0.3 | lr 0.02315\n",
            "[02/06 03:26:54] __main__ INFO: [25344/50000] acc 0.997881 | loss 0.213464 | sup loss 0.00662189 | ssl loss 0.689475 | mask 1 | sec/iter 0.218917 | ssl coef 0.3 | lr 0.02301\n",
            "[02/06 03:27:55] __main__ INFO: [25600/50000] acc 0.998474 | loss 0.217722 | sup loss 0.00696112 | ssl loss 0.702536 | mask 1 | sec/iter 0.22055 | ssl coef 0.3 | lr 0.02287\n",
            "[02/06 03:28:53] __main__ INFO: [25856/50000] acc 0.997976 | loss 0.231119 | sup loss 0.0171543 | ssl loss 0.713217 | mask 1 | sec/iter 0.223 | ssl coef 0.3 | lr 0.02274\n",
            "[02/06 03:29:51] __main__ INFO: [26112/50000] acc 0.995429 | loss 0.219728 | sup loss 0.00910797 | ssl loss 0.702068 | mask 1 | sec/iter 0.224744 | ssl coef 0.3 | lr 0.0226\n",
            "[02/06 03:30:51] __main__ INFO: [26368/50000] acc 0.999043 | loss 0.212091 | sup loss 0.00355425 | ssl loss 0.695122 | mask 1 | sec/iter 0.222049 | ssl coef 0.3 | lr 0.02246\n",
            "[02/06 03:31:49] __main__ INFO: [26624/50000] acc 0.996191 | loss 0.21746 | sup loss 0.0116344 | ssl loss 0.686087 | mask 1 | sec/iter 0.219138 | ssl coef 0.3 | lr 0.02232\n",
            "[02/06 03:32:47] __main__ INFO: [26880/50000] acc 0.999938 | loss 0.222454 | sup loss 0.00226706 | ssl loss 0.733958 | mask 1 | sec/iter 0.22123 | ssl coef 0.3 | lr 0.02218\n",
            "[02/06 03:37:14] __main__ INFO: Iteration: 27001, subset selection finished, takes 238.95. \n",
            "[02/06 03:37:47] __main__ INFO: [27136/50000] acc 0.998666 | loss 0.224939 | sup loss 0.00385959 | ssl loss 0.736931 | mask 1 | sec/iter 0.228773 | ssl coef 0.3 | lr 0.02203\n",
            "[02/06 03:38:45] __main__ INFO: [27392/50000] acc 0.999865 | loss 0.208593 | sup loss 0.00500112 | ssl loss 0.678639 | mask 1 | sec/iter 0.216369 | ssl coef 0.3 | lr 0.02189\n",
            "[02/06 03:39:44] __main__ INFO: [27648/50000] acc 0.999739 | loss 0.208708 | sup loss 0.00309303 | ssl loss 0.685384 | mask 1 | sec/iter 0.224345 | ssl coef 0.3 | lr 0.02175\n",
            "[02/06 03:40:44] __main__ INFO: [27904/50000] acc 0.997677 | loss 0.225406 | sup loss 0.00588079 | ssl loss 0.73175 | mask 1 | sec/iter 0.227839 | ssl coef 0.3 | lr 0.0216\n",
            "[02/06 03:41:43] __main__ INFO: [28160/50000] acc 0.996294 | loss 0.236896 | sup loss 0.00934334 | ssl loss 0.758508 | mask 1 | sec/iter 0.218749 | ssl coef 0.3 | lr 0.02145\n",
            "[02/06 03:42:41] __main__ INFO: [28416/50000] acc 0.998258 | loss 0.208689 | sup loss 0.00878971 | ssl loss 0.666332 | mask 1 | sec/iter 0.217774 | ssl coef 0.3 | lr 0.0213\n",
            "[02/06 03:43:40] __main__ INFO: [28672/50000] acc 0.997311 | loss 0.214587 | sup loss 0.00818394 | ssl loss 0.688009 | mask 1 | sec/iter 0.220944 | ssl coef 0.3 | lr 0.02116\n",
            "[02/06 03:44:40] __main__ INFO: [28928/50000] acc 0.998275 | loss 0.19537 | sup loss 0.00369287 | ssl loss 0.638924 | mask 1 | sec/iter 0.219606 | ssl coef 0.3 | lr 0.02101\n",
            "[02/06 03:48:52] __main__ INFO: Iteration: 29001, subset selection finished, takes 235.46. \n",
            "[02/06 03:49:36] __main__ INFO: [29184/50000] acc 0.999976 | loss 0.199868 | sup loss 0.0017662 | ssl loss 0.660339 | mask 1 | sec/iter 0.224321 | ssl coef 0.3 | lr 0.02085\n",
            "[02/06 03:50:35] __main__ INFO: [29440/50000] acc 0.998791 | loss 0.202546 | sup loss 0.00300943 | ssl loss 0.665121 | mask 1 | sec/iter 0.249326 | ssl coef 0.3 | lr 0.0207\n",
            "[02/06 03:51:34] __main__ INFO: [29696/50000] acc 0.998099 | loss 0.208335 | sup loss 0.00602045 | ssl loss 0.674383 | mask 1 | sec/iter 0.220385 | ssl coef 0.3 | lr 0.02055\n",
            "[02/06 03:52:32] __main__ INFO: [29952/50000] acc 0.99912 | loss 0.191473 | sup loss 0.00419975 | ssl loss 0.624242 | mask 1 | sec/iter 0.223899 | ssl coef 0.3 | lr 0.02039\n",
            "[02/06 03:52:43] __main__ INFO: test\n",
            "[02/06 03:55:34] __main__ INFO: test loss 1.239559 | test acc. 0.802400 | raw acc. 0.802400\n",
            "[02/06 03:56:22] __main__ INFO: [30208/50000] acc 0.996211 | loss 0.196991 | sup loss 0.0158881 | ssl loss 0.603677 | mask 1 | sec/iter 0.224788 | ssl coef 0.3 | lr 0.02024\n",
            "[02/06 03:57:22] __main__ INFO: [30464/50000] acc 0.99964 | loss 0.188458 | sup loss 0.00190861 | ssl loss 0.621832 | mask 1 | sec/iter 0.244156 | ssl coef 0.3 | lr 0.02008\n",
            "[02/06 03:58:22] __main__ INFO: [30720/50000] acc 0.999604 | loss 0.200725 | sup loss 0.00264028 | ssl loss 0.660284 | mask 1 | sec/iter 0.223938 | ssl coef 0.3 | lr 0.01993\n",
            "[02/06 03:59:20] __main__ INFO: [30976/50000] acc 0.999989 | loss 0.188453 | sup loss 0.00193558 | ssl loss 0.621725 | mask 1 | sec/iter 0.220072 | ssl coef 0.3 | lr 0.01977\n",
            "[02/06 04:03:22] __main__ INFO: Iteration: 31001, subset selection finished, takes 235.93. \n",
            "[02/06 04:04:20] __main__ INFO: [31232/50000] acc 0.9997 | loss 0.185265 | sup loss 0.0015516 | ssl loss 0.61238 | mask 1 | sec/iter 0.226507 | ssl coef 0.3 | lr 0.01961\n",
            "[02/06 04:05:19] __main__ INFO: [31488/50000] acc 0.995698 | loss 0.214286 | sup loss 0.0126958 | ssl loss 0.671966 | mask 1 | sec/iter 0.220029 | ssl coef 0.3 | lr 0.01945\n",
            "[02/06 04:06:18] __main__ INFO: [31744/50000] acc 1 | loss 0.180258 | sup loss 0.00160632 | ssl loss 0.595505 | mask 1 | sec/iter 0.219095 | ssl coef 0.3 | lr 0.01929\n",
            "[02/06 04:07:17] __main__ INFO: [32000/50000] acc 0.99591 | loss 0.193772 | sup loss 0.00748279 | ssl loss 0.620965 | mask 1 | sec/iter 0.220257 | ssl coef 0.3 | lr 0.01912\n",
            "[02/06 04:08:17] __main__ INFO: [32256/50000] acc 0.999992 | loss 0.185521 | sup loss 0.00195385 | ssl loss 0.611892 | mask 1 | sec/iter 0.213792 | ssl coef 0.3 | lr 0.01896\n",
            "[02/06 04:09:15] __main__ INFO: [32512/50000] acc 0.999426 | loss 0.176764 | sup loss 0.00155947 | ssl loss 0.584014 | mask 1 | sec/iter 0.216634 | ssl coef 0.3 | lr 0.0188\n",
            "[02/06 04:10:14] __main__ INFO: [32768/50000] acc 0.998281 | loss 0.177596 | sup loss 0.00657659 | ssl loss 0.570066 | mask 1 | sec/iter 0.22192 | ssl coef 0.3 | lr 0.01863\n",
            "[02/06 04:15:04] __main__ INFO: Iteration: 33001, subset selection finished, takes 235.01. \n",
            "[02/06 04:15:11] __main__ INFO: [33024/50000] acc 0.999984 | loss 0.193916 | sup loss 0.000901048 | ssl loss 0.643382 | mask 1 | sec/iter 0.229168 | ssl coef 0.3 | lr 0.01847\n",
            "[02/06 04:16:10] __main__ INFO: [33280/50000] acc 0.999052 | loss 0.210953 | sup loss 0.00526028 | ssl loss 0.685643 | mask 1 | sec/iter 0.223995 | ssl coef 0.3 | lr 0.0183\n",
            "[02/06 04:17:10] __main__ INFO: [33536/50000] acc 0.999782 | loss 0.190876 | sup loss 0.00335496 | ssl loss 0.62507 | mask 1 | sec/iter 0.22166 | ssl coef 0.3 | lr 0.01813\n",
            "[02/06 04:18:10] __main__ INFO: [33792/50000] acc 0.998033 | loss 0.192462 | sup loss 0.00483803 | ssl loss 0.625413 | mask 1 | sec/iter 0.222589 | ssl coef 0.3 | lr 0.01796\n",
            "[02/06 04:19:09] __main__ INFO: [34048/50000] acc 1 | loss 0.185252 | sup loss 0.00146554 | ssl loss 0.61262 | mask 1 | sec/iter 0.2248 | ssl coef 0.3 | lr 0.01779\n",
            "[02/06 04:20:07] __main__ INFO: [34304/50000] acc 0.999272 | loss 0.172891 | sup loss 0.00299177 | ssl loss 0.566332 | mask 1 | sec/iter 0.224745 | ssl coef 0.3 | lr 0.01762\n",
            "[02/06 04:21:06] __main__ INFO: [34560/50000] acc 0.996643 | loss 0.204689 | sup loss 0.00942645 | ssl loss 0.650874 | mask 1 | sec/iter 0.219073 | ssl coef 0.3 | lr 0.01745\n",
            "[02/06 04:22:06] __main__ INFO: [34816/50000] acc 0.999938 | loss 0.168885 | sup loss 0.000833062 | ssl loss 0.560172 | mask 1 | sec/iter 0.220482 | ssl coef 0.3 | lr 0.01728\n",
            "[02/06 04:26:46] __main__ INFO: Iteration: 35001, subset selection finished, takes 237.03. \n",
            "[02/06 04:27:04] __main__ INFO: [35072/50000] acc 0.998828 | loss 0.2081 | sup loss 0.00328753 | ssl loss 0.682708 | mask 1 | sec/iter 0.222522 | ssl coef 0.3 | lr 0.01711\n",
            "[02/06 04:28:04] __main__ INFO: [35328/50000] acc 0.996661 | loss 0.191831 | sup loss 0.00648345 | ssl loss 0.617826 | mask 1 | sec/iter 0.230667 | ssl coef 0.3 | lr 0.01693\n",
            "[02/06 04:29:06] __main__ INFO: [35584/50000] acc 1 | loss 0.177184 | sup loss 0.00127096 | ssl loss 0.586376 | mask 1 | sec/iter 0.225632 | ssl coef 0.3 | lr 0.01676\n",
            "[02/06 04:30:06] __main__ INFO: [35840/50000] acc 0.999156 | loss 0.174702 | sup loss 0.00264458 | ssl loss 0.573526 | mask 1 | sec/iter 0.224113 | ssl coef 0.3 | lr 0.01658\n",
            "[02/06 04:31:06] __main__ INFO: [36096/50000] acc 0.997846 | loss 0.199062 | sup loss 0.00534461 | ssl loss 0.645726 | mask 1 | sec/iter 0.223037 | ssl coef 0.3 | lr 0.01641\n",
            "[02/06 04:32:07] __main__ INFO: [36352/50000] acc 1 | loss 0.17619 | sup loss 0.00162378 | ssl loss 0.581887 | mask 1 | sec/iter 0.232641 | ssl coef 0.3 | lr 0.01623\n",
            "[02/06 04:33:07] __main__ INFO: [36608/50000] acc 0.998389 | loss 0.17589 | sup loss 0.00286428 | ssl loss 0.576752 | mask 1 | sec/iter 0.22131 | ssl coef 0.3 | lr 0.01605\n",
            "[02/06 04:34:07] __main__ INFO: [36864/50000] acc 0.999938 | loss 0.17882 | sup loss 0.000841123 | ssl loss 0.593262 | mask 1 | sec/iter 0.225343 | ssl coef 0.3 | lr 0.01587\n",
            "[02/06 04:38:37] __main__ INFO: Iteration: 37001, subset selection finished, takes 237.84. \n",
            "[02/06 04:39:08] __main__ INFO: [37120/50000] acc 0.998519 | loss 0.18596 | sup loss 0.00529339 | ssl loss 0.602222 | mask 1 | sec/iter 0.223264 | ssl coef 0.3 | lr 0.01569\n",
            "[02/06 04:40:07] __main__ INFO: [37376/50000] acc 0.999757 | loss 0.191533 | sup loss 0.0026041 | ssl loss 0.629764 | mask 1 | sec/iter 0.221781 | ssl coef 0.3 | lr 0.01551\n",
            "[02/06 04:41:06] __main__ INFO: [37632/50000] acc 0.996003 | loss 0.168231 | sup loss 0.00628088 | ssl loss 0.539835 | mask 1 | sec/iter 0.228589 | ssl coef 0.3 | lr 0.01533\n",
            "[02/06 04:42:05] __main__ INFO: [37888/50000] acc 0.999208 | loss 0.161752 | sup loss 0.00301347 | ssl loss 0.529129 | mask 1 | sec/iter 0.22391 | ssl coef 0.3 | lr 0.01515\n",
            "[02/06 04:43:05] __main__ INFO: [38144/50000] acc 0.998318 | loss 0.158366 | sup loss 0.0125952 | ssl loss 0.485901 | mask 1 | sec/iter 0.220094 | ssl coef 0.3 | lr 0.01497\n",
            "[02/06 04:44:04] __main__ INFO: [38400/50000] acc 0.998543 | loss 0.155301 | sup loss 0.00389731 | ssl loss 0.504678 | mask 1 | sec/iter 0.223421 | ssl coef 0.3 | lr 0.01478\n",
            "[02/06 04:45:04] __main__ INFO: [38656/50000] acc 0.999699 | loss 0.163328 | sup loss 0.0020813 | ssl loss 0.53749 | mask 1 | sec/iter 0.227255 | ssl coef 0.3 | lr 0.0146\n",
            "[02/06 04:46:04] __main__ INFO: [38912/50000] acc 1 | loss 0.153063 | sup loss 0.0007557 | ssl loss 0.507691 | mask 1 | sec/iter 0.236555 | ssl coef 0.3 | lr 0.01441\n",
            "[02/06 04:50:22] __main__ INFO: Iteration: 39001, subset selection finished, takes 237.25. \n",
            "[02/06 04:51:03] __main__ INFO: [39168/50000] acc 0.999262 | loss 0.178024 | sup loss 0.00487081 | ssl loss 0.577176 | mask 1 | sec/iter 0.225278 | ssl coef 0.3 | lr 0.01423\n",
            "[02/06 04:52:02] __main__ INFO: [39424/50000] acc 0.999905 | loss 0.182417 | sup loss 0.000496575 | ssl loss 0.606401 | mask 1 | sec/iter 0.224976 | ssl coef 0.3 | lr 0.01404\n",
            "[02/06 04:53:01] __main__ INFO: [39680/50000] acc 1 | loss 0.167416 | sup loss 0.000862199 | ssl loss 0.555179 | mask 1 | sec/iter 0.221998 | ssl coef 0.3 | lr 0.01386\n",
            "[02/06 04:54:01] __main__ INFO: [39936/50000] acc 1 | loss 0.148217 | sup loss 0.000334685 | ssl loss 0.49294 | mask 1 | sec/iter 0.221883 | ssl coef 0.3 | lr 0.01367\n",
            "[02/06 04:54:16] __main__ INFO: test\n",
            "[02/06 04:57:06] __main__ INFO: test loss 1.236839 | test acc. 0.809600 | raw acc. 0.809600\n",
            "[02/06 04:57:51] __main__ INFO: [40192/50000] acc 0.999535 | loss 0.170814 | sup loss 0.00205511 | ssl loss 0.562529 | mask 1 | sec/iter 0.220192 | ssl coef 0.3 | lr 0.01348\n",
            "[02/06 04:58:50] __main__ INFO: [40448/50000] acc 0.999947 | loss 0.164608 | sup loss 0.00215332 | ssl loss 0.541515 | mask 1 | sec/iter 0.217887 | ssl coef 0.3 | lr 0.01329\n",
            "[02/06 04:59:48] __main__ INFO: [40704/50000] acc 0.999863 | loss 0.152054 | sup loss 0.000800106 | ssl loss 0.504181 | mask 1 | sec/iter 0.221825 | ssl coef 0.3 | lr 0.0131\n",
            "[02/06 05:00:49] __main__ INFO: [40960/50000] acc 1 | loss 0.163316 | sup loss 0.000544207 | ssl loss 0.542573 | mask 1 | sec/iter 0.223499 | ssl coef 0.3 | lr 0.01291\n",
            "[02/06 05:04:57] __main__ INFO: Iteration: 41001, subset selection finished, takes 239.01. \n",
            "[02/06 05:05:49] __main__ INFO: [41216/50000] acc 0.999862 | loss 0.164212 | sup loss 0.00158686 | ssl loss 0.542083 | mask 1 | sec/iter 0.220562 | ssl coef 0.3 | lr 0.01272\n",
            "[02/06 05:06:48] __main__ INFO: [41472/50000] acc 0.999707 | loss 0.156835 | sup loss 0.00117255 | ssl loss 0.518876 | mask 1 | sec/iter 0.222853 | ssl coef 0.3 | lr 0.01253\n",
            "[02/06 05:07:47] __main__ INFO: [41728/50000] acc 0.999802 | loss 0.189978 | sup loss 0.00318155 | ssl loss 0.622655 | mask 1 | sec/iter 0.221963 | ssl coef 0.3 | lr 0.01234\n",
            "[02/06 05:08:47] __main__ INFO: [41984/50000] acc 0.998477 | loss 0.161705 | sup loss 0.00250057 | ssl loss 0.530681 | mask 1 | sec/iter 0.224728 | ssl coef 0.3 | lr 0.01214\n",
            "[02/06 05:09:46] __main__ INFO: [42240/50000] acc 1 | loss 0.169295 | sup loss 0.000947014 | ssl loss 0.561158 | mask 1 | sec/iter 0.225744 | ssl coef 0.3 | lr 0.01195\n",
            "[02/06 05:10:44] __main__ INFO: [42496/50000] acc 0.999886 | loss 0.15236 | sup loss 0.000774205 | ssl loss 0.505285 | mask 1 | sec/iter 0.220504 | ssl coef 0.3 | lr 0.01176\n",
            "[02/06 05:11:43] __main__ INFO: [42752/50000] acc 0.999999 | loss 0.149446 | sup loss 0.000316945 | ssl loss 0.497097 | mask 1 | sec/iter 0.22349 | ssl coef 0.3 | lr 0.01156\n",
            "[02/06 05:16:36] __main__ INFO: Iteration: 43001, subset selection finished, takes 234.47. \n",
            "[02/06 05:16:39] __main__ INFO: [43008/50000] acc 0.996988 | loss 0.167808 | sup loss 0.0131665 | ssl loss 0.515473 | mask 1 | sec/iter 0.243146 | ssl coef 0.3 | lr 0.01137\n",
            "[02/06 05:17:38] __main__ INFO: [43264/50000] acc 0.998688 | loss 0.165921 | sup loss 0.00161405 | ssl loss 0.547691 | mask 1 | sec/iter 0.219174 | ssl coef 0.3 | lr 0.01117\n",
            "[02/06 05:18:36] __main__ INFO: [43520/50000] acc 1 | loss 0.159405 | sup loss 0.000516129 | ssl loss 0.52963 | mask 1 | sec/iter 0.221832 | ssl coef 0.3 | lr 0.01098\n",
            "[02/06 05:19:34] __main__ INFO: [43776/50000] acc 0.998804 | loss 0.157789 | sup loss 0.00180559 | ssl loss 0.519945 | mask 1 | sec/iter 0.224149 | ssl coef 0.3 | lr 0.01078\n",
            "[02/06 05:20:34] __main__ INFO: [44032/50000] acc 0.999981 | loss 0.156712 | sup loss 0.000821207 | ssl loss 0.519635 | mask 1 | sec/iter 0.216637 | ssl coef 0.3 | lr 0.01058\n",
            "[02/06 05:21:33] __main__ INFO: [44288/50000] acc 1 | loss 0.137924 | sup loss 0.000542292 | ssl loss 0.457938 | mask 1 | sec/iter 0.225094 | ssl coef 0.3 | lr 0.01038\n",
            "[02/06 05:22:32] __main__ INFO: [44544/50000] acc 0.999924 | loss 0.150239 | sup loss 0.00085121 | ssl loss 0.497961 | mask 1 | sec/iter 0.216348 | ssl coef 0.3 | lr 0.01018\n",
            "[02/06 05:23:30] __main__ INFO: [44800/50000] acc 0.999984 | loss 0.139618 | sup loss 0.000617232 | ssl loss 0.463335 | mask 1 | sec/iter 0.220373 | ssl coef 0.3 | lr 0.009986\n",
            "[02/06 05:28:17] __main__ INFO: Iteration: 45001, subset selection finished, takes 238.68. \n",
            "[02/06 05:28:32] __main__ INFO: [45056/50000] acc 1 | loss 0.159508 | sup loss 0.000557639 | ssl loss 0.529834 | mask 1 | sec/iter 0.22028 | ssl coef 0.3 | lr 0.009787\n",
            "[02/06 05:29:30] __main__ INFO: [45312/50000] acc 0.999726 | loss 0.168881 | sup loss 0.000942571 | ssl loss 0.559794 | mask 1 | sec/iter 0.221213 | ssl coef 0.3 | lr 0.009587\n",
            "[02/06 05:30:29] __main__ INFO: [45568/50000] acc 0.999542 | loss 0.158802 | sup loss 0.00105404 | ssl loss 0.525828 | mask 1 | sec/iter 0.223678 | ssl coef 0.3 | lr 0.009387\n",
            "[02/06 05:31:29] __main__ INFO: [45824/50000] acc 0.998886 | loss 0.160345 | sup loss 0.00185613 | ssl loss 0.528296 | mask 1 | sec/iter 0.23432 | ssl coef 0.3 | lr 0.009186\n",
            "[02/06 05:32:28] __main__ INFO: [46080/50000] acc 1 | loss 0.152797 | sup loss 0.000209391 | ssl loss 0.508624 | mask 1 | sec/iter 0.219053 | ssl coef 0.3 | lr 0.008985\n",
            "[02/06 05:33:28] __main__ INFO: [46336/50000] acc 1 | loss 0.148727 | sup loss 0.000443963 | ssl loss 0.494277 | mask 1 | sec/iter 0.219166 | ssl coef 0.3 | lr 0.008783\n",
            "[02/06 05:34:26] __main__ INFO: [46592/50000] acc 0.99998 | loss 0.158594 | sup loss 0.000628017 | ssl loss 0.526552 | mask 1 | sec/iter 0.220893 | ssl coef 0.3 | lr 0.008581\n",
            "[02/06 05:35:27] __main__ INFO: [46848/50000] acc 0.999884 | loss 0.155085 | sup loss 0.000392671 | ssl loss 0.51564 | mask 1 | sec/iter 0.22182 | ssl coef 0.3 | lr 0.008379\n",
            "[02/06 05:39:58] __main__ INFO: Iteration: 47001, subset selection finished, takes 235.32. \n",
            "[02/06 05:40:24] __main__ INFO: [47104/50000] acc 0.999997 | loss 0.14668 | sup loss 0.000587404 | ssl loss 0.486977 | mask 1 | sec/iter 0.235622 | ssl coef 0.3 | lr 0.008176\n",
            "[02/06 05:41:23] __main__ INFO: [47360/50000] acc 1 | loss 0.149662 | sup loss 0.00131877 | ssl loss 0.494478 | mask 1 | sec/iter 0.222431 | ssl coef 0.3 | lr 0.007972\n",
            "[02/06 05:42:22] __main__ INFO: [47616/50000] acc 0.999946 | loss 0.140744 | sup loss 0.00060351 | ssl loss 0.467134 | mask 1 | sec/iter 0.224101 | ssl coef 0.3 | lr 0.007769\n",
            "[02/06 05:43:22] __main__ INFO: [47872/50000] acc 1 | loss 0.151703 | sup loss 0.000187133 | ssl loss 0.505052 | mask 1 | sec/iter 0.222589 | ssl coef 0.3 | lr 0.007564\n",
            "[02/06 05:44:21] __main__ INFO: [48128/50000] acc 0.999997 | loss 0.153296 | sup loss 0.000324679 | ssl loss 0.509904 | mask 1 | sec/iter 0.218162 | ssl coef 0.3 | lr 0.00736\n",
            "[02/06 05:45:19] __main__ INFO: [48384/50000] acc 1 | loss 0.143003 | sup loss 0.000342082 | ssl loss 0.475535 | mask 1 | sec/iter 0.223473 | ssl coef 0.3 | lr 0.007155\n",
            "[02/06 05:46:18] __main__ INFO: [48640/50000] acc 1 | loss 0.132384 | sup loss 0.000259908 | ssl loss 0.440413 | mask 1 | sec/iter 0.22045 | ssl coef 0.3 | lr 0.00695\n",
            "[02/06 05:47:18] __main__ INFO: [48896/50000] acc 1 | loss 0.127211 | sup loss 0.000433651 | ssl loss 0.422592 | mask 1 | sec/iter 0.222821 | ssl coef 0.3 | lr 0.006744\n",
            "[02/06 05:51:40] __main__ INFO: Iteration: 49001, subset selection finished, takes 236.97. \n",
            "[02/06 05:52:16] __main__ INFO: [49152/50000] acc 1 | loss 0.150775 | sup loss 0.000204899 | ssl loss 0.501899 | mask 1 | sec/iter 0.217397 | ssl coef 0.3 | lr 0.006539\n",
            "[02/06 05:53:15] __main__ INFO: [49408/50000] acc 0.999406 | loss 0.14727 | sup loss 0.00170157 | ssl loss 0.485229 | mask 1 | sec/iter 0.222979 | ssl coef 0.3 | lr 0.006332\n",
            "[02/06 05:54:13] __main__ INFO: [49664/50000] acc 1 | loss 0.135964 | sup loss 0.000226439 | ssl loss 0.452458 | mask 1 | sec/iter 0.218653 | ssl coef 0.3 | lr 0.006126\n",
            "[02/06 05:55:13] __main__ INFO: [49920/50000] acc 0.998324 | loss 0.147269 | sup loss 0.00270468 | ssl loss 0.48188 | mask 1 | sec/iter 0.219832 | ssl coef 0.3 | lr 0.005919\n",
            "[02/06 05:55:32] __main__ INFO: test\n",
            "[02/06 05:58:22] __main__ INFO: test loss 1.246967 | test acc. 0.819500 | raw acc. 0.819500\n",
            "[02/06 05:58:23] __main__ INFO: Total Time taken: 11168.279504\n",
            "[02/06 05:58:23] __main__ INFO: Subset Selection Time: 0.000000\n",
            "[02/06 05:58:23] __main__ INFO: mean test acc. over last 1 checkpoints: 0.819500\n",
            "[02/06 05:58:23] __main__ INFO: mean test acc. for raw model over last 1 checkpoints: 0.819500\n",
            "[02/06 05:58:23] __main__ INFO: mean test acc. over last 10 checkpoints: 0.802400\n",
            "[02/06 05:58:23] __main__ INFO: mean test acc. for raw model over last 10 checkpoints: 0.802400\n",
            "[02/06 05:58:23] __main__ INFO: mean test acc. over last 20 checkpoints: 0.802400\n",
            "[02/06 05:58:23] __main__ INFO: mean test acc. for raw model over last 20 checkpoints: 0.802400\n",
            "[02/06 05:58:23] __main__ INFO: mean test acc. over last 50 checkpoints: 0.802400\n",
            "[02/06 05:58:23] __main__ INFO: mean test acc. for raw model over last 50 checkpoints: 0.802400\n"
          ]
        }
      ],
      "source": [
        "model.train()\n",
        "logger.info(model)\n",
        "\n",
        "# init meter for metrics logging\n",
        "metric_meter = Meter()\n",
        "test_acc_list = []\n",
        "raw_acc_list = []\n",
        "logger.info(\"training\")\n",
        "\n",
        "iter_count = 1\n",
        "subset_selection_time = 0\n",
        "training_time = 0\n",
        "\n",
        "# Start training until maximum number of iterations are reached\n",
        "while iter_count <= max_iteration:\n",
        "    lt_loader = DataLoader(\n",
        "        lt_data,\n",
        "        cfg.dataloader.l_batch_size,\n",
        "        sampler=dataset_utils.InfiniteSampler(len(lt_data), len(list(\n",
        "            ult_loader.batch_sampler)) * cfg.dataloader.l_batch_size),\n",
        "        num_workers=cfg.dataloader.num_workers\n",
        "    )\n",
        "\n",
        "    logger.debug(\"Data loader iteration count is: {0:d}\".format(len(list(ult_loader.batch_sampler))))\n",
        "    # Enumerate on batches of labeled and unlabeled data. \n",
        "    # Note that the ult_loader enumerates only on subsets of unlabeled data selected by RETRIEVE\n",
        "    for batch_idx, (l_data, ul_data) in enumerate(zip(lt_loader, ult_loader)):\n",
        "        batch_start_time = time.time()\n",
        "        if iter_count > max_iteration:\n",
        "            break\n",
        "        l_aug, labels = l_data\n",
        "        ul_w_aug, ul_s_aug, _, weights = ul_data\n",
        "        if cfg.dataset.feature in ['ood', 'classimb']:\n",
        "            ood = True\n",
        "        else:\n",
        "            ood = False\n",
        "        params = param_update(\n",
        "                cfg, iter_count, model, teacher_model, optimizer, ssl_alg,\n",
        "                consistency, l_aug.to(cfg.train_args.device), ul_w_aug.to(cfg.train_args.device),\n",
        "                ul_s_aug.to(cfg.train_args.device), labels.to(cfg.train_args.device),\n",
        "                average_model, weights=weights.to(cfg.train_args.device), ood=ood)\n",
        "        training_time += (time.time() - batch_start_time)\n",
        "        \n",
        "        # moving average for reporting losses and accuracy\n",
        "        metric_meter.add(params, ignores=[\"coef\"])\n",
        "        \n",
        "        # display losses every cfg.disp iterations\n",
        "        if ((iter_count + 1) % cfg.train_args.disp) == 0:\n",
        "            state = metric_meter.state(\n",
        "                header=f'[{iter_count + 1}/{max_iteration}]',\n",
        "                footer=f'ssl coef {params[\"coef\"]:.4g} | lr {optimizer.param_groups[0][\"lr\"]:.4g}'\n",
        "            )\n",
        "            logger.info(state)\n",
        "        lr_scheduler.step()\n",
        "        \n",
        "        # Checkpoint model at regular intervals\n",
        "        if ((iter_count + 1) % cfg.ckpt.checkpoint) == 0 or (iter_count + 1) == max_iteration:\n",
        "            with torch.no_grad():\n",
        "                if cfg.ssl_eval_args.weight_average:\n",
        "                    eval_model = average_model\n",
        "                else:\n",
        "                    eval_model = model\n",
        "                logger.info(\"test\")\n",
        "                mean_raw_acc, mean_test_acc, mean_test_loss = evaluation(model, eval_model, test_loader,\n",
        "                                                                              cfg.train_args.device)\n",
        "                logger.info(\"test loss %f | test acc. %f | raw acc. %f\", mean_test_loss, mean_test_acc,\n",
        "                            mean_raw_acc)\n",
        "                test_acc_list.append(mean_test_acc)\n",
        "                raw_acc_list.append(mean_raw_acc)\n",
        "            torch.save(model.state_dict(), os.path.join(cfg.train_args.results_dir, \"model_checkpoint.pth\"))\n",
        "            torch.save(optimizer.state_dict(),\n",
        "                        os.path.join(cfg.train_args.results_dir, \"optimizer_checkpoint.pth\"))\n",
        "        iter_count += 1\n",
        "numpy.save(os.path.join(cfg.train_args.results_dir, \"evaluation_results\"), test_acc_list)\n",
        "numpy.save(os.path.join(cfg.train_args.results_dir, \"raw_results\"), raw_acc_list)\n",
        "logger.info(\"Total Time taken: %f\", training_time + subset_selection_time)\n",
        "logger.info(\"Subset Selection Time: %f\", subset_selection_time)\n",
        "accuracies = {}\n",
        "for i in [1, 10, 20, 50]:\n",
        "    logger.info(\"mean test acc. over last %d checkpoints: %f\", i, numpy.median(test_acc_list[-i:]))\n",
        "    logger.info(\"mean test acc. for raw model over last %d checkpoints: %f\", i, numpy.median(raw_acc_list[-i:]))\n",
        "    accuracies[f\"last{i}\"] = numpy.median(test_acc_list[-i:])\n",
        "\n",
        "with open(os.path.join(cfg.train_args.results_dir, \"results.json\"), \"w\") as f:\n",
        "    json.dump(accuracies, f, sort_keys=True)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JSHYpMlR9JII"
      },
      "source": [
        "# Using default SSL training loop directly\n",
        "\n",
        "We have incorporated the above training loop in train_ssl.py file of CORDS which can be used by directly importing the TrainClassifier class from train_ssl function as follows:\n",
        "\n",
        "```\n",
        "from train_ssl import TrainClassifier\n",
        "```\n",
        "\n",
        "Importing Semi-Supervised learning default training loop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "_7--My3U9JIJ"
      },
      "outputs": [],
      "source": [
        "from train_ssl import TrainClassifier"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C90z1L0j9JIK"
      },
      "source": [
        "### Loading default RETRIEVE config file for CIFAR10 dataset\n",
        "\n",
        "We can load other subset selection strategies like CRAIG, GradMatch, Random for CIFAR10 dataset by loading their respective config files.\n",
        "\n",
        "Here we give an example of instantiating a SSL training loop using RETRIEVE config file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "f8QhXaz99JIK",
        "outputId": "38b5b55e-c9d9-41c1-8c57-36b8bd16db34"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[02/06 05:58:23] train_ssl INFO: DotMap(setting='SSL', dataset=DotMap(name='cifar10', root='../data', feature='dss', type='pre-defined', num_labels=4000, val_ratio=0.1, ood_ratio=0.5, random_split=False, whiten=False, zca=True, labeled_aug='WA', unlabeled_aug='WA', wa='t.t.f', strong_aug=False), dataloader=DotMap(shuffle=True, pin_memory=True, num_workers=8, l_batch_size=50, ul_batch_size=50), model=DotMap(architecture='wrn', type='pre-defined', numclasses=10), ckpt=DotMap(is_load=False, is_save=True, checkpoint_model='model.ckpt', checkpoint_optimizer='optimizer.ckpt', start_iter=None, checkpoint=10000), loss=DotMap(type='CrossEntropyLoss', use_sigmoid=False), optimizer=DotMap(type='sgd', momentum=0.9, lr=0.03, weight_decay=0, nesterov=True, tsa=False, tsa_schedule='linear'), scheduler=DotMap(lr_decay='cos', warmup_iter=0), ssl_args=DotMap(alg='vat', coef=0.3, ema_teacher=False, ema_teacher_warmup=False, ema_teacher_factor=0.999, ema_apply_wd=False, em=0, threshold=None, sharpen=None, temp_softmax=None, consis='ce', eps=6, xi=1e-06, vat_iter=1), ssl_eval_args=DotMap(weight_average=False, wa_ema_factor=0.999, wa_apply_wd=False), dss_args=DotMap(type='RETRIEVE-Warm', fraction=0.1, select_every=20, kappa=0.5, linear_layer=False, selection_type='Supervised', greedy='Stochastic', valid=True), train_args=DotMap(iteration=500000, max_iter=-1, device='cuda', results_dir='results/', disp=256, seed=96))\n"
          ]
        }
      ],
      "source": [
        "fraction = 0.1\n",
        "retrieve_config_file = '/content/cords/configs/SSL/config_retrieve-warm_vat_cifar10.py'\n",
        "\n",
        "from cords.utils.config_utils import load_config_data\n",
        "\n",
        "cfg = load_config_data(retrieve_config_file)\n",
        "retrieve_trn = TrainClassifier(cfg)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7cR3wmtZ9JIK"
      },
      "source": [
        "### Default config args can be modified in the following manner\n",
        "\n",
        "We can modify the default arguments of the config file by just assigning them a new file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "WVcxHn0g9JIL"
      },
      "outputs": [],
      "source": [
        "retrieve_trn.cfg.train_args.disp = 256\n",
        "retrieve_trn.cfg.train_args.device = 'cuda'\n",
        "retrieve_trn.cfg.dss_args.fraction = fraction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "19rABQo19JIL"
      },
      "source": [
        "### Start the training process"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "d6ecca6b2a9d411f82f2dd555f3c2407"
          ]
        },
        "id": "CxbRePmJ9JIM",
        "outputId": "2a7bb7f7-20a5-450c-da93-0b0b2d5928a5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[02/06 05:58:23] train_ssl INFO: load dataset\n",
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ../data/cifar-10-python.tar.gz\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d6ecca6b2a9d411f82f2dd555f3c2407",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/170498071 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting ../data/cifar-10-python.tar.gz to ../data\n",
            "[02/06 05:58:38] train_ssl INFO: number of :\n",
            "             training data: 50000\n",
            "             labeled data: 4000\n",
            "             unlabeled data: 50000\n",
            "             validation data: 0\n",
            "             test data: 10000\n",
            "[02/06 05:59:01] train_ssl INFO: labeled augmentation\n",
            "[02/06 05:59:01] train_ssl INFO: Compose(\n",
            "    ToPILImage()\n",
            "    RandomHorizontalFlip(p=0.5)\n",
            "    RandomCrop(size=(32, 32), padding=4)\n",
            "    ToTensor()\n",
            "    GCN(multiplier=55, eps=1e-10)\n",
            "    ZCA()\n",
            ")\n",
            "[02/06 05:59:01] train_ssl INFO: unlabeled augmentation\n",
            "[02/06 05:59:01] train_ssl INFO: Compose(\n",
            "    ToPILImage()\n",
            "    RandomHorizontalFlip(p=0.5)\n",
            "    RandomCrop(size=(32, 32), padding=4)\n",
            "    ToTensor()\n",
            "    GCN(multiplier=55, eps=1e-10)\n",
            "    ZCA()\n",
            ")\n",
            "[02/06 05:59:01] train_ssl INFO: WideResNet(\n",
            "  (feature_extractor): Sequential(\n",
            "    (0): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "    (1): _Residual(\n",
            "      (pre_act): Sequential(\n",
            "        (0): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (1): LeakyReLU(negative_slope=0.1)\n",
            "      )\n",
            "      (identity): Conv2d(16, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (layer): Sequential(\n",
            "        (0): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (2): LeakyReLU(negative_slope=0.1)\n",
            "        (3): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      )\n",
            "    )\n",
            "    (2): _Residual(\n",
            "      (pre_act): Identity()\n",
            "      (identity): Identity()\n",
            "      (layer): Sequential(\n",
            "        (0): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (1): LeakyReLU(negative_slope=0.1)\n",
            "        (2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (4): LeakyReLU(negative_slope=0.1)\n",
            "        (5): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      )\n",
            "    )\n",
            "    (3): _Residual(\n",
            "      (pre_act): Identity()\n",
            "      (identity): Identity()\n",
            "      (layer): Sequential(\n",
            "        (0): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (1): LeakyReLU(negative_slope=0.1)\n",
            "        (2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (4): LeakyReLU(negative_slope=0.1)\n",
            "        (5): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      )\n",
            "    )\n",
            "    (4): _Residual(\n",
            "      (pre_act): Identity()\n",
            "      (identity): Identity()\n",
            "      (layer): Sequential(\n",
            "        (0): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (1): LeakyReLU(negative_slope=0.1)\n",
            "        (2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (4): LeakyReLU(negative_slope=0.1)\n",
            "        (5): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      )\n",
            "    )\n",
            "    (5): _Residual(\n",
            "      (pre_act): Identity()\n",
            "      (identity): Conv2d(32, 64, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "      (layer): Sequential(\n",
            "        (0): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (1): LeakyReLU(negative_slope=0.1)\n",
            "        (2): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "        (3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (4): LeakyReLU(negative_slope=0.1)\n",
            "        (5): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      )\n",
            "    )\n",
            "    (6): _Residual(\n",
            "      (pre_act): Identity()\n",
            "      (identity): Identity()\n",
            "      (layer): Sequential(\n",
            "        (0): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (1): LeakyReLU(negative_slope=0.1)\n",
            "        (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (4): LeakyReLU(negative_slope=0.1)\n",
            "        (5): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      )\n",
            "    )\n",
            "    (7): _Residual(\n",
            "      (pre_act): Identity()\n",
            "      (identity): Identity()\n",
            "      (layer): Sequential(\n",
            "        (0): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (1): LeakyReLU(negative_slope=0.1)\n",
            "        (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (4): LeakyReLU(negative_slope=0.1)\n",
            "        (5): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      )\n",
            "    )\n",
            "    (8): _Residual(\n",
            "      (pre_act): Identity()\n",
            "      (identity): Identity()\n",
            "      (layer): Sequential(\n",
            "        (0): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (1): LeakyReLU(negative_slope=0.1)\n",
            "        (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (4): LeakyReLU(negative_slope=0.1)\n",
            "        (5): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      )\n",
            "    )\n",
            "    (9): _Residual(\n",
            "      (pre_act): Identity()\n",
            "      (identity): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "      (layer): Sequential(\n",
            "        (0): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (1): LeakyReLU(negative_slope=0.1)\n",
            "        (2): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "        (3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (4): LeakyReLU(negative_slope=0.1)\n",
            "        (5): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      )\n",
            "    )\n",
            "    (10): _Residual(\n",
            "      (pre_act): Identity()\n",
            "      (identity): Identity()\n",
            "      (layer): Sequential(\n",
            "        (0): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (1): LeakyReLU(negative_slope=0.1)\n",
            "        (2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (4): LeakyReLU(negative_slope=0.1)\n",
            "        (5): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      )\n",
            "    )\n",
            "    (11): _Residual(\n",
            "      (pre_act): Identity()\n",
            "      (identity): Identity()\n",
            "      (layer): Sequential(\n",
            "        (0): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (1): LeakyReLU(negative_slope=0.1)\n",
            "        (2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (4): LeakyReLU(negative_slope=0.1)\n",
            "        (5): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      )\n",
            "    )\n",
            "    (12): _Residual(\n",
            "      (pre_act): Identity()\n",
            "      (identity): Identity()\n",
            "      (layer): Sequential(\n",
            "        (0): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (1): LeakyReLU(negative_slope=0.1)\n",
            "        (2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (4): LeakyReLU(negative_slope=0.1)\n",
            "        (5): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      )\n",
            "    )\n",
            "    (13): _Residual(\n",
            "      (pre_act): Identity()\n",
            "      (identity): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "      (layer): Sequential(\n",
            "        (0): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (1): LeakyReLU(negative_slope=0.1)\n",
            "        (2): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "        (3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (4): LeakyReLU(negative_slope=0.1)\n",
            "        (5): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      )\n",
            "    )\n",
            "    (14): _Residual(\n",
            "      (pre_act): Identity()\n",
            "      (identity): Identity()\n",
            "      (layer): Sequential(\n",
            "        (0): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (1): LeakyReLU(negative_slope=0.1)\n",
            "        (2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (4): LeakyReLU(negative_slope=0.1)\n",
            "        (5): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      )\n",
            "    )\n",
            "    (15): _Residual(\n",
            "      (pre_act): Identity()\n",
            "      (identity): Identity()\n",
            "      (layer): Sequential(\n",
            "        (0): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (1): LeakyReLU(negative_slope=0.1)\n",
            "        (2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (4): LeakyReLU(negative_slope=0.1)\n",
            "        (5): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      )\n",
            "    )\n",
            "    (16): _Residual(\n",
            "      (pre_act): Identity()\n",
            "      (identity): Identity()\n",
            "      (layer): Sequential(\n",
            "        (0): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (1): LeakyReLU(negative_slope=0.1)\n",
            "        (2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (4): LeakyReLU(negative_slope=0.1)\n",
            "        (5): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      )\n",
            "    )\n",
            "    (17): _Residual(\n",
            "      (pre_act): Identity()\n",
            "      (identity): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "      (layer): Sequential(\n",
            "        (0): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (1): LeakyReLU(negative_slope=0.1)\n",
            "        (2): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "        (3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (4): LeakyReLU(negative_slope=0.1)\n",
            "        (5): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      )\n",
            "    )\n",
            "    (18): _Residual(\n",
            "      (pre_act): Identity()\n",
            "      (identity): Identity()\n",
            "      (layer): Sequential(\n",
            "        (0): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (1): LeakyReLU(negative_slope=0.1)\n",
            "        (2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (4): LeakyReLU(negative_slope=0.1)\n",
            "        (5): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      )\n",
            "    )\n",
            "    (19): _Residual(\n",
            "      (pre_act): Identity()\n",
            "      (identity): Identity()\n",
            "      (layer): Sequential(\n",
            "        (0): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (1): LeakyReLU(negative_slope=0.1)\n",
            "        (2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (4): LeakyReLU(negative_slope=0.1)\n",
            "        (5): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      )\n",
            "    )\n",
            "    (20): _Residual(\n",
            "      (pre_act): Identity()\n",
            "      (identity): Identity()\n",
            "      (layer): Sequential(\n",
            "        (0): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (1): LeakyReLU(negative_slope=0.1)\n",
            "        (2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (4): LeakyReLU(negative_slope=0.1)\n",
            "        (5): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      )\n",
            "    )\n",
            "    (21): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (22): LeakyReLU(negative_slope=0.1)\n",
            "  )\n",
            "  (classifier): Sequential(\n",
            "    (0): Linear(in_features=512, out_features=10, bias=True)\n",
            "  )\n",
            ")\n",
            "[02/06 05:59:01] train_ssl INFO: training\n",
            "[02/06 06:00:03] train_ssl INFO: [256/50000] acc 0.505201 | loss 2.10294 | sup loss 1.41943 | ssl loss 2.27838 | mask 1 | sec/iter 0.219709 | ssl coef 0.3 | lr 0.03\n",
            "[02/06 06:01:02] train_ssl INFO: [512/50000] acc 0.582062 | loss 1.79517 | sup loss 1.18802 | ssl loss 2.02381 | mask 1 | sec/iter 0.223509 | ssl coef 0.3 | lr 0.03\n",
            "[02/06 06:02:02] train_ssl INFO: [768/50000] acc 0.630163 | loss 1.57611 | sup loss 1.00178 | ssl loss 1.91445 | mask 1 | sec/iter 0.226975 | ssl coef 0.3 | lr 0.02999\n",
            "[02/06 06:03:03] train_ssl INFO: [1024/50000] acc 0.688973 | loss 1.45483 | sup loss 0.88477 | ssl loss 1.90021 | mask 1 | sec/iter 0.225257 | ssl coef 0.3 | lr 0.02999\n",
            "[02/06 06:04:02] train_ssl INFO: [1280/50000] acc 0.709913 | loss 1.39445 | sup loss 0.84696 | ssl loss 1.82495 | mask 1 | sec/iter 0.226282 | ssl coef 0.3 | lr 0.02998\n",
            "[02/06 06:05:01] train_ssl INFO: [1536/50000] acc 0.762837 | loss 1.18517 | sup loss 0.647114 | ssl loss 1.79354 | mask 1 | sec/iter 0.223541 | ssl coef 0.3 | lr 0.02997\n",
            "[02/06 06:06:00] train_ssl INFO: [1792/50000] acc 0.783269 | loss 1.13595 | sup loss 0.608097 | ssl loss 1.75949 | mask 1 | sec/iter 0.225045 | ssl coef 0.3 | lr 0.02996\n",
            "[02/06 06:07:01] train_ssl INFO: [2048/50000] acc 0.779027 | loss 1.11311 | sup loss 0.584867 | ssl loss 1.76081 | mask 1 | sec/iter 0.223084 | ssl coef 0.3 | lr 0.02995\n",
            "[02/06 06:08:00] train_ssl INFO: [2304/50000] acc 0.79085 | loss 1.09832 | sup loss 0.559842 | ssl loss 1.79493 | mask 1 | sec/iter 0.224888 | ssl coef 0.3 | lr 0.02994\n",
            "[02/06 06:09:00] train_ssl INFO: [2560/50000] acc 0.816985 | loss 1.00935 | sup loss 0.504561 | ssl loss 1.68264 | mask 1 | sec/iter 0.220741 | ssl coef 0.3 | lr 0.02993\n",
            "[02/06 06:09:59] train_ssl INFO: [2816/50000] acc 0.845985 | loss 0.919083 | sup loss 0.427839 | ssl loss 1.63748 | mask 1 | sec/iter 0.223047 | ssl coef 0.3 | lr 0.02991\n",
            "[02/06 06:10:59] train_ssl INFO: [3072/50000] acc 0.903742 | loss 0.791908 | sup loss 0.307919 | ssl loss 1.6133 | mask 1 | sec/iter 0.217374 | ssl coef 0.3 | lr 0.02989\n",
            "[02/06 06:11:58] train_ssl INFO: [3328/50000] acc 0.859188 | loss 0.861625 | sup loss 0.3657 | ssl loss 1.65308 | mask 1 | sec/iter 0.226135 | ssl coef 0.3 | lr 0.02987\n",
            "[02/06 06:12:57] train_ssl INFO: [3584/50000] acc 0.909886 | loss 0.728254 | sup loss 0.255404 | ssl loss 1.57617 | mask 1 | sec/iter 0.220847 | ssl coef 0.3 | lr 0.02985\n",
            "[02/06 06:13:55] train_ssl INFO: [3840/50000] acc 0.894246 | loss 0.7418 | sup loss 0.282314 | ssl loss 1.53162 | mask 1 | sec/iter 0.22271 | ssl coef 0.3 | lr 0.02983\n",
            "[02/06 06:14:55] train_ssl INFO: [4096/50000] acc 0.914931 | loss 0.695241 | sup loss 0.235962 | ssl loss 1.53093 | mask 1 | sec/iter 0.223207 | ssl coef 0.3 | lr 0.02981\n",
            "[02/06 06:15:53] train_ssl INFO: [4352/50000] acc 0.945124 | loss 0.608721 | sup loss 0.171548 | ssl loss 1.45724 | mask 1 | sec/iter 0.223771 | ssl coef 0.3 | lr 0.02979\n",
            "[02/06 06:16:53] train_ssl INFO: [4608/50000] acc 0.929971 | loss 0.653376 | sup loss 0.196049 | ssl loss 1.52442 | mask 1 | sec/iter 0.223124 | ssl coef 0.3 | lr 0.02976\n",
            "[02/06 06:17:52] train_ssl INFO: [4864/50000] acc 0.931161 | loss 0.599939 | sup loss 0.177541 | ssl loss 1.40799 | mask 1 | sec/iter 0.223712 | ssl coef 0.3 | lr 0.02973\n",
            "[02/06 06:18:54] train_ssl INFO: [5120/50000] acc 0.940285 | loss 0.606652 | sup loss 0.17107 | ssl loss 1.45194 | mask 1 | sec/iter 0.230647 | ssl coef 0.3 | lr 0.0297\n",
            "[02/06 06:19:53] train_ssl INFO: [5376/50000] acc 0.958379 | loss 0.564939 | sup loss 0.133636 | ssl loss 1.43767 | mask 1 | sec/iter 0.224554 | ssl coef 0.3 | lr 0.02967\n",
            "[02/06 06:20:53] train_ssl INFO: [5632/50000] acc 0.960414 | loss 0.531819 | sup loss 0.110267 | ssl loss 1.40517 | mask 1 | sec/iter 0.22279 | ssl coef 0.3 | lr 0.02964\n",
            "[02/06 06:21:52] train_ssl INFO: [5888/50000] acc 0.951883 | loss 0.515155 | sup loss 0.121426 | ssl loss 1.31243 | mask 1 | sec/iter 0.223608 | ssl coef 0.3 | lr 0.02961\n",
            "[02/06 06:22:54] train_ssl INFO: [6144/50000] acc 0.958791 | loss 0.529922 | sup loss 0.116941 | ssl loss 1.3766 | mask 1 | sec/iter 0.228269 | ssl coef 0.3 | lr 0.02957\n",
            "[02/06 06:23:53] train_ssl INFO: [6400/50000] acc 0.955835 | loss 0.505599 | sup loss 0.105312 | ssl loss 1.33429 | mask 1 | sec/iter 0.226825 | ssl coef 0.3 | lr 0.02954\n",
            "[02/06 06:24:53] train_ssl INFO: [6656/50000] acc 0.962223 | loss 0.528407 | sup loss 0.115983 | ssl loss 1.37475 | mask 1 | sec/iter 0.221815 | ssl coef 0.3 | lr 0.0295\n",
            "[02/06 06:25:53] train_ssl INFO: [6912/50000] acc 0.963004 | loss 0.496556 | sup loss 0.10236 | ssl loss 1.31399 | mask 1 | sec/iter 0.227731 | ssl coef 0.3 | lr 0.02946\n",
            "[02/06 06:26:54] train_ssl INFO: [7168/50000] acc 0.971521 | loss 0.470345 | sup loss 0.0809535 | ssl loss 1.29797 | mask 1 | sec/iter 0.230588 | ssl coef 0.3 | lr 0.02942\n",
            "[02/06 06:27:54] train_ssl INFO: [7424/50000] acc 0.976045 | loss 0.448931 | sup loss 0.0594045 | ssl loss 1.29842 | mask 1 | sec/iter 0.224937 | ssl coef 0.3 | lr 0.02938\n",
            "[02/06 06:28:54] train_ssl INFO: [7680/50000] acc 0.960595 | loss 0.470368 | sup loss 0.107159 | ssl loss 1.2107 | mask 1 | sec/iter 0.224469 | ssl coef 0.3 | lr 0.02933\n",
            "[02/06 06:29:53] train_ssl INFO: [7936/50000] acc 0.978986 | loss 0.423256 | sup loss 0.0555595 | ssl loss 1.22565 | mask 1 | sec/iter 0.223981 | ssl coef 0.3 | lr 0.02929\n",
            "[02/06 06:30:54] train_ssl INFO: [8192/50000] acc 0.973062 | loss 0.449892 | sup loss 0.0777922 | ssl loss 1.24033 | mask 1 | sec/iter 0.251664 | ssl coef 0.3 | lr 0.02924\n",
            "[02/06 06:31:53] train_ssl INFO: [8448/50000] acc 0.974581 | loss 0.440787 | sup loss 0.0620431 | ssl loss 1.26248 | mask 1 | sec/iter 0.223332 | ssl coef 0.3 | lr 0.0292\n",
            "[02/06 06:32:52] train_ssl INFO: [8704/50000] acc 0.978753 | loss 0.421071 | sup loss 0.0502947 | ssl loss 1.23592 | mask 1 | sec/iter 0.222336 | ssl coef 0.3 | lr 0.02915\n",
            "[02/06 06:33:51] train_ssl INFO: [8960/50000] acc 0.985758 | loss 0.38229 | sup loss 0.0398294 | ssl loss 1.14154 | mask 1 | sec/iter 0.219432 | ssl coef 0.3 | lr 0.0291\n",
            "[02/06 06:34:49] train_ssl INFO: [9216/50000] acc 0.989973 | loss 0.434527 | sup loss 0.0412352 | ssl loss 1.31097 | mask 1 | sec/iter 0.235347 | ssl coef 0.3 | lr 0.02904\n",
            "[02/06 06:35:49] train_ssl INFO: [9472/50000] acc 0.978843 | loss 0.423942 | sup loss 0.0612952 | ssl loss 1.20882 | mask 1 | sec/iter 0.218111 | ssl coef 0.3 | lr 0.02899\n",
            "[02/06 06:36:47] train_ssl INFO: [9728/50000] acc 0.989719 | loss 0.387897 | sup loss 0.0319265 | ssl loss 1.18657 | mask 1 | sec/iter 0.226658 | ssl coef 0.3 | lr 0.02893\n",
            "[02/06 06:37:46] train_ssl INFO: [9984/50000] acc 0.989526 | loss 0.405575 | sup loss 0.0367442 | ssl loss 1.22944 | mask 1 | sec/iter 0.223526 | ssl coef 0.3 | lr 0.02888\n",
            "[02/06 06:37:50] train_ssl INFO: test\n",
            "[02/06 06:40:40] train_ssl INFO: test loss 1.172095 | test acc. 0.761800 | raw acc. 0.761800\n",
            "[02/06 06:41:36] train_ssl INFO: [10240/50000] acc 0.975751 | loss 0.406885 | sup loss 0.0616893 | ssl loss 1.15065 | mask 1 | sec/iter 0.224247 | ssl coef 0.3 | lr 0.02882\n",
            "[02/06 06:42:35] train_ssl INFO: [10496/50000] acc 0.976784 | loss 0.414285 | sup loss 0.0585478 | ssl loss 1.18579 | mask 1 | sec/iter 0.221761 | ssl coef 0.3 | lr 0.02876\n",
            "[02/06 06:43:36] train_ssl INFO: [10752/50000] acc 0.979824 | loss 0.405738 | sup loss 0.061928 | ssl loss 1.14603 | mask 1 | sec/iter 0.22703 | ssl coef 0.3 | lr 0.0287\n",
            "[02/06 06:44:36] train_ssl INFO: [11008/50000] acc 0.98561 | loss 0.360747 | sup loss 0.0452624 | ssl loss 1.05161 | mask 1 | sec/iter 0.224936 | ssl coef 0.3 | lr 0.02864\n",
            "[02/06 06:45:36] train_ssl INFO: [11264/50000] acc 0.98527 | loss 0.389974 | sup loss 0.0481715 | ssl loss 1.13934 | mask 1 | sec/iter 0.226182 | ssl coef 0.3 | lr 0.02857\n",
            "[02/06 06:46:35] train_ssl INFO: [11520/50000] acc 0.993405 | loss 0.365147 | sup loss 0.0293656 | ssl loss 1.11927 | mask 1 | sec/iter 0.223161 | ssl coef 0.3 | lr 0.02851\n",
            "[02/06 06:47:36] train_ssl INFO: [11776/50000] acc 0.991227 | loss 0.337616 | sup loss 0.0239099 | ssl loss 1.04569 | mask 1 | sec/iter 0.22588 | ssl coef 0.3 | lr 0.02844\n",
            "[02/06 06:48:37] train_ssl INFO: [12032/50000] acc 0.991917 | loss 0.34962 | sup loss 0.0231669 | ssl loss 1.08818 | mask 1 | sec/iter 0.226044 | ssl coef 0.3 | lr 0.02837\n",
            "[02/06 06:49:36] train_ssl INFO: [12288/50000] acc 0.990312 | loss 0.342157 | sup loss 0.0333286 | ssl loss 1.02943 | mask 1 | sec/iter 0.228311 | ssl coef 0.3 | lr 0.02831\n",
            "[02/06 06:50:36] train_ssl INFO: [12544/50000] acc 0.982122 | loss 0.356751 | sup loss 0.0462683 | ssl loss 1.03494 | mask 1 | sec/iter 0.222444 | ssl coef 0.3 | lr 0.02823\n",
            "[02/06 06:51:37] train_ssl INFO: [12800/50000] acc 0.992215 | loss 0.334448 | sup loss 0.019735 | ssl loss 1.04904 | mask 1 | sec/iter 0.224012 | ssl coef 0.3 | lr 0.02816\n",
            "[02/06 06:52:37] train_ssl INFO: [13056/50000] acc 0.989982 | loss 0.339929 | sup loss 0.0282443 | ssl loss 1.03895 | mask 1 | sec/iter 0.221363 | ssl coef 0.3 | lr 0.02809\n",
            "[02/06 06:53:36] train_ssl INFO: [13312/50000] acc 0.993921 | loss 0.312626 | sup loss 0.01643 | ssl loss 0.987321 | mask 1 | sec/iter 0.222604 | ssl coef 0.3 | lr 0.02801\n",
            "[02/06 06:54:35] train_ssl INFO: [13568/50000] acc 0.985637 | loss 0.341347 | sup loss 0.0361046 | ssl loss 1.01747 | mask 1 | sec/iter 0.222511 | ssl coef 0.3 | lr 0.02794\n",
            "[02/06 06:55:38] train_ssl INFO: [13824/50000] acc 0.985074 | loss 0.370517 | sup loss 0.0342498 | ssl loss 1.12089 | mask 1 | sec/iter 0.224485 | ssl coef 0.3 | lr 0.02786\n",
            "[02/06 06:56:40] train_ssl INFO: [14080/50000] acc 0.987742 | loss 0.341021 | sup loss 0.0297179 | ssl loss 1.03768 | mask 1 | sec/iter 0.233232 | ssl coef 0.3 | lr 0.02778\n",
            "[02/06 06:57:41] train_ssl INFO: [14336/50000] acc 0.991251 | loss 0.332268 | sup loss 0.0209527 | ssl loss 1.03772 | mask 1 | sec/iter 0.22975 | ssl coef 0.3 | lr 0.0277\n",
            "[02/06 06:58:44] train_ssl INFO: [14592/50000] acc 0.991741 | loss 0.331227 | sup loss 0.0294085 | ssl loss 1.00606 | mask 1 | sec/iter 0.247763 | ssl coef 0.3 | lr 0.02762\n",
            "[02/06 06:59:46] train_ssl INFO: [14848/50000] acc 0.995821 | loss 0.317581 | sup loss 0.0202274 | ssl loss 0.991178 | mask 1 | sec/iter 0.228084 | ssl coef 0.3 | lr 0.02754\n",
            "[02/06 07:00:47] train_ssl INFO: [15104/50000] acc 0.997141 | loss 0.287676 | sup loss 0.00997457 | ssl loss 0.925672 | mask 1 | sec/iter 0.228503 | ssl coef 0.3 | lr 0.02745\n",
            "[02/06 07:01:47] train_ssl INFO: [15360/50000] acc 0.994875 | loss 0.294623 | sup loss 0.0148539 | ssl loss 0.932562 | mask 1 | sec/iter 0.225297 | ssl coef 0.3 | lr 0.02737\n",
            "[02/06 07:02:48] train_ssl INFO: [15616/50000] acc 0.996574 | loss 0.283308 | sup loss 0.0136974 | ssl loss 0.898703 | mask 1 | sec/iter 0.243686 | ssl coef 0.3 | lr 0.02728\n",
            "[02/06 07:03:49] train_ssl INFO: [15872/50000] acc 0.996775 | loss 0.27976 | sup loss 0.0092034 | ssl loss 0.901855 | mask 1 | sec/iter 0.228705 | ssl coef 0.3 | lr 0.02719\n",
            "[02/06 07:04:49] train_ssl INFO: [16128/50000] acc 0.99339 | loss 0.313189 | sup loss 0.0276517 | ssl loss 0.951792 | mask 1 | sec/iter 0.227246 | ssl coef 0.3 | lr 0.0271\n",
            "[02/06 07:05:48] train_ssl INFO: [16384/50000] acc 0.99084 | loss 0.319118 | sup loss 0.0295435 | ssl loss 0.965247 | mask 1 | sec/iter 0.228707 | ssl coef 0.3 | lr 0.02701\n",
            "[02/06 07:06:48] train_ssl INFO: [16640/50000] acc 0.994511 | loss 0.291055 | sup loss 0.0171756 | ssl loss 0.912933 | mask 1 | sec/iter 0.222751 | ssl coef 0.3 | lr 0.02692\n",
            "[02/06 07:07:49] train_ssl INFO: [16896/50000] acc 0.993313 | loss 0.264007 | sup loss 0.0154744 | ssl loss 0.828442 | mask 1 | sec/iter 0.225421 | ssl coef 0.3 | lr 0.02682\n",
            "[02/06 07:08:50] train_ssl INFO: [17152/50000] acc 0.99641 | loss 0.276026 | sup loss 0.0117417 | ssl loss 0.880949 | mask 1 | sec/iter 0.227421 | ssl coef 0.3 | lr 0.02673\n",
            "[02/06 07:09:50] train_ssl INFO: [17408/50000] acc 0.997224 | loss 0.291391 | sup loss 0.00699734 | ssl loss 0.94798 | mask 1 | sec/iter 0.231396 | ssl coef 0.3 | lr 0.02663\n",
            "[02/06 07:10:49] train_ssl INFO: [17664/50000] acc 0.991652 | loss 0.284926 | sup loss 0.0169778 | ssl loss 0.893161 | mask 1 | sec/iter 0.226237 | ssl coef 0.3 | lr 0.02653\n",
            "[02/06 07:11:50] train_ssl INFO: [17920/50000] acc 0.998561 | loss 0.261691 | sup loss 0.00543034 | ssl loss 0.854201 | mask 1 | sec/iter 0.22486 | ssl coef 0.3 | lr 0.02643\n",
            "[02/06 07:12:50] train_ssl INFO: [18176/50000] acc 0.994991 | loss 0.278416 | sup loss 0.0169883 | ssl loss 0.871426 | mask 1 | sec/iter 0.226048 | ssl coef 0.3 | lr 0.02633\n",
            "[02/06 07:13:50] train_ssl INFO: [18432/50000] acc 0.996593 | loss 0.276759 | sup loss 0.0154846 | ssl loss 0.870915 | mask 1 | sec/iter 0.224631 | ssl coef 0.3 | lr 0.02623\n",
            "[02/06 07:14:49] train_ssl INFO: [18688/50000] acc 0.992675 | loss 0.261717 | sup loss 0.0211229 | ssl loss 0.801982 | mask 1 | sec/iter 0.224603 | ssl coef 0.3 | lr 0.02613\n",
            "[02/06 07:15:50] train_ssl INFO: [18944/50000] acc 0.998865 | loss 0.232876 | sup loss 0.00596808 | ssl loss 0.75636 | mask 1 | sec/iter 0.226011 | ssl coef 0.3 | lr 0.02602\n",
            "[02/06 07:16:49] train_ssl INFO: [19200/50000] acc 0.990736 | loss 0.276781 | sup loss 0.0185218 | ssl loss 0.860865 | mask 1 | sec/iter 0.224916 | ssl coef 0.3 | lr 0.02592\n",
            "[02/06 07:17:48] train_ssl INFO: [19456/50000] acc 0.988028 | loss 0.282687 | sup loss 0.0356571 | ssl loss 0.823433 | mask 1 | sec/iter 0.22357 | ssl coef 0.3 | lr 0.02581\n",
            "[02/06 07:18:47] train_ssl INFO: [19712/50000] acc 0.9952 | loss 0.252199 | sup loss 0.0128165 | ssl loss 0.797941 | mask 1 | sec/iter 0.222677 | ssl coef 0.3 | lr 0.0257\n",
            "[02/06 07:19:49] train_ssl INFO: [19968/50000] acc 0.995296 | loss 0.250212 | sup loss 0.0112897 | ssl loss 0.796409 | mask 1 | sec/iter 0.226413 | ssl coef 0.3 | lr 0.02559\n",
            "[02/06 07:19:56] train_ssl INFO: test\n",
            "[02/06 07:22:47] train_ssl INFO: test loss 1.213224 | test acc. 0.789800 | raw acc. 0.789800\n",
            "[02/06 07:23:40] train_ssl INFO: [20224/50000] acc 0.995657 | loss 0.267376 | sup loss 0.0154007 | ssl loss 0.839917 | mask 1 | sec/iter 0.222217 | ssl coef 0.3 | lr 0.02548\n",
            "[02/06 07:24:39] train_ssl INFO: [20480/50000] acc 0.989843 | loss 0.270874 | sup loss 0.0279714 | ssl loss 0.809676 | mask 1 | sec/iter 0.22013 | ssl coef 0.3 | lr 0.02537\n",
            "[02/06 07:25:39] train_ssl INFO: [20736/50000] acc 0.998618 | loss 0.244188 | sup loss 0.00527609 | ssl loss 0.796374 | mask 1 | sec/iter 0.226284 | ssl coef 0.3 | lr 0.02526\n",
            "[02/06 07:26:39] train_ssl INFO: [20992/50000] acc 0.995887 | loss 0.254071 | sup loss 0.0165171 | ssl loss 0.791845 | mask 1 | sec/iter 0.224321 | ssl coef 0.3 | lr 0.02514\n",
            "[02/06 07:27:40] train_ssl INFO: [21248/50000] acc 0.999777 | loss 0.231418 | sup loss 0.00352525 | ssl loss 0.759643 | mask 1 | sec/iter 0.22449 | ssl coef 0.3 | lr 0.02503\n"
          ]
        }
      ],
      "source": [
        "retrieve_trn.train()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "0ce29baa7efa4ec89f35443105e24a0f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "28aacca06cf746fc9f2ee35b78939190": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2a6c5eaed1454183bf5bfd0b7da0236d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "484fedf4fb1d4096b94b4a6c0dcb8404": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_639eafff1225470d8e5deae79ff51688",
              "IPY_MODEL_efc76f5d95b042109ef6d5b9e1d25d7d",
              "IPY_MODEL_f973931cc1144eb682d19f7bf75b05e3"
            ],
            "layout": "IPY_MODEL_d6349c6388814efa83ffb477a93ee986"
          }
        },
        "639eafff1225470d8e5deae79ff51688": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b383e1bb4724406b9a69cbaf29676c4d",
            "placeholder": "​",
            "style": "IPY_MODEL_9879665b0e5d4bfbbcf2dd3ed36a810b",
            "value": "100%"
          }
        },
        "75401a58ef2a4c2883e8505d74c2d0fe": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9879665b0e5d4bfbbcf2dd3ed36a810b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b383e1bb4724406b9a69cbaf29676c4d": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d6349c6388814efa83ffb477a93ee986": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "efc76f5d95b042109ef6d5b9e1d25d7d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_75401a58ef2a4c2883e8505d74c2d0fe",
            "max": 170498071,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_0ce29baa7efa4ec89f35443105e24a0f",
            "value": 170498071
          }
        },
        "f973931cc1144eb682d19f7bf75b05e3": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_28aacca06cf746fc9f2ee35b78939190",
            "placeholder": "​",
            "style": "IPY_MODEL_2a6c5eaed1454183bf5bfd0b7da0236d",
            "value": " 170498071/170498071 [00:10&lt;00:00, 15509225.56it/s]"
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}